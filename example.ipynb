{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# IEOR4575 Project\n",
    "Instructor: Professor Shipra Agrawal\\\n",
    "Contributors: Yunhao Tang, Abhi Gupta\n",
    "\n",
    "## State-Action Description\n",
    "\n",
    "State s is an array with give components\n",
    "\n",
    "* s[0]:  constraint matrix $A$of the current LP ($\\max  -c^Tx \\text{ s.t. }Ax \\le  b$) . Dimension is $m \\times n$. See by printing s[0].shape. Here $n$ is the (fixed) number of variables. For instances of size 60 by 60 used in the above command, $n$ will remain fixed as 60. And $m$ is the current number of constraints. Initially, $m$ is to the number of constraints in the IP instance. (For instances generated with --num-c=60, $m$ is 60 at the first step).  But $m$ will increase by one in every step of the episode as one new constraint (cut) is added on taking an action.\n",
    "* s[1]: rhs $b$ for the current LP ($Ax\\le b$). Dimension same as the number $m$ in matrix A.\n",
    "* s[2]: coefficient vector $c$ from the LP objective ($-c^Tx$). Dimension same as the number of variables, i.e., $n$.\n",
    "* s[3],  s[4]: Gomory cuts available in the current round of Gomory's cutting plane algorithm. Each cut $i$ is of the form $D_i x\\le d_i$.   s[3] gives the matrix $D$ (of dimension $k \\times n$) of cuts and s[4] gives the rhs $d$ (of dimension $k$). The number of cuts $k$ available in each round changes, you can find it out by printing the size of last component of state, i.e., s[4].size or s[-1].size.\n",
    "\n",
    "## Example\n",
    "You can use the Jupyter notebook example.ipnyb on colab to familiarize yourself with the cutting plane environment that we have built for you.\n",
    "\n",
    "If you are using an offline environment (not colab) you can use example.py file.\n",
    "```\n",
    "$ python example.py\n",
    "```\n",
    "\n",
    "## TASK\n",
    "Train on two training environments: easy and hard:\n",
    " 10 instances and\n",
    "100 instances\n",
    "of size n=60, m=60, episode length 50\n",
    "\n",
    "Submit Code + Report of at most 5 pages, with algorithm, plots etc.\n",
    "Additional pages can be used to provide supplementary material which may or may not be reviewed, as necessary.\n",
    "\n",
    "These two can be loaded by using the following two configs (see example.py). Each mode is characterized by a set of parameters that define the cutting plane environment.\n",
    "\n",
    "The easy setup defines the environment as follows:\n",
    "```\n",
    "easy_config = {\n",
    "    \"load_dir\"        : 'instances/train_10_n60_m60',\n",
    "    \"idx_list\"        : list(range(10)),\n",
    "    \"timelimit\"       : 50,\n",
    "    \"reward_type\"     : 'obj'\n",
    "}\n",
    "```\n",
    "For your reference, the maximum total sum of rewards achievable in any given episode in the easy mode is 2.947 +- 0.5469.\n",
    "\n",
    "\n",
    "The hard setup defines the environment as follows:\n",
    "```\n",
    "hard_config = {\n",
    "    \"load_dir\"        : 'instances/train_100_n60_m60',\n",
    "    \"idx_list\"        : list(range(99)),\n",
    "    \"timelimit\"       : 50,\n",
    "    \"reward_type\"     : 'obj'\n",
    "}\n",
    "```\n",
    "On average, the maximum total sum of rewards achievable in any given episode in the hard mode is 2.985 +- 0.8427. But, the achieving close to 1 reward (i.e. closing the integrality gap by 1) is a reasonably good performance and can be achieved with what we have learned in this course.\n",
    "\n",
    "The main difference between the easy and hard modes is the number of training instances. Easy contains 10 instances while hard contains 100. Please read the ```example.py``` script would further details about what these environment parameters mean.\n",
    "\n",
    "## Generating New Instances (Optional)\n",
    "\n",
    "To make sure your algorithm generalizes to instances beyond those in the instances folder, you can create new environments with random IP instances and train/test on those. To generate new instances, run the following script. This will create 100 new instances with 60 constraints and 60 variables.\n",
    "\n",
    "You can show generalization performance on new instances that you didn't train for, for extra credit. You can also show other aspects of your solution like robustness to size of instances.\n",
    "\n",
    "```\n",
    "$ python generate_randomip.py --num-v 60 --num-c 60 --num-instances 100\n",
    "```\n",
    "\n",
    "The above instances will be saved in a directory named 'instances/randomip_n60_m60'. Then, we can load instances into gym env and train a cutting agent. The following code loads the 50th instance and run an episode with horizon 50:\n",
    "\n",
    "```\n",
    "python testgymenv.py --timelimit 50 --instance-idx 50 --instance-name randomip_n60_m60\n",
    "```\n",
    "\n",
    "We should see the printing of step information till the episode ends.\n",
    "\n",
    "If you do not provide --instance-idx, then the environment will load random instance out of the 100 instances in every episode. It is sometimes easier to train on a single instance to start with, instead of a pool of instances.\n",
    "\n",
    "## Notes\n",
    "\n",
    "- The env is not exactly equivalent to gym env where the state and action spaces are fixed. Here, the size of state and action space vary over time. The RL agent needs to handle variable state-action spaces.\n",
    "- The env uses python interface and computes optimal LP solution using Gurobi. If you are not using colab, make sure Gurobi is installed and license is valid. There is a free academic license as well as an online course limited use license available. See the installation instructions below. You don't need to do this if you are using example jupyter notebook in colab.\n",
    "\n",
    "## Installation\n",
    "```\n",
    "$ conda install -c gurobi gurobi\n",
    "```\n",
    "\n",
    "In addition, you need an academic license from gurobi. After getting the license, go to the license page.\n",
    "\n",
    "(https://www.gurobi.com/downloads/end-user-license-agreement-academic/)\n",
    "\n",
    " In order to activate the license, you will need to run the **grbgetkey** command with the license key written there. After this step, you can use the `ieor4575` environment that you have used for labs to complete the class project.\n",
    "\n",
    "## WandB for Visualizaition\n",
    "Class labs have made extensive use of wandb to familiarize you with some great machine learning visualization tools. You are encouraged to use wandb in the development of this project. See example notebook for the project name to use. You can move your best runs to the leaderboard.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## See README.md file for further details about the project and the environment.\n",
    "\n",
    "### State-Action Description\n",
    "\n",
    "### State\n",
    "State s is an array with give components\n",
    "\n",
    "* s[0]:  constraint matrix $A$of the current LP ($\\max  -c^Tx \\text{ s.t. }Ax \\le  b$) . Dimension is $m \\times n$. See by printing s[0].shape. Here $n$ is the (fixed) number of variables. For instances of size 60 by 60 used in the above command, $n$ will remain fixed as 60. And $m$ is the current number of constraints. Initially, $m$ is to the number of constraints in the IP instance. (For instances generated with --num-c=60, $m$ is 60 at the first step).  But $m$ will increase by one in every step of the episode as one new constraint (cut) is added on taking an action.\n",
    "* s[1]: rhs $b$ for the current LP ($Ax\\le b$). Dimension same as the number $m$ in matrix A.\n",
    "* s[2]: coefficient vector $c$ from the LP objective ($-c^Tx$). Dimension same as the number of variables, i.e., $n$.\n",
    "* s[3],  s[4]: Gomory cuts available in the current round of Gomory's cutting plane algorithm. Each cut $i$ is of the form $D_i x\\le d_i$.   s[3] gives the matrix $D$ (of dimension $k \\times n$) of cuts and s[4] gives the rhs $d$ (of dimension $k$). The number of cuts $k$ available in each round changes, you can find it out by printing the size of last component of state, i.e., s[4].size or s[-1].size.\n",
    "\n",
    "### Actions\n",
    "There are k=s[4].size actions available in each state $s$, with $i^{th}$ action corresponding to the $i^{th}$ cut with inequality $D_i x\\le d_i$ in $s[3], s[4]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33757,
     "status": "ok",
     "timestamp": 1669737544513,
     "user": {
      "displayName": "Shipra Agrawal",
      "userId": "12760872725468885671"
     },
     "user_tz": 300
    },
    "id": "1H51OwGYq2Tp",
    "outputId": "9b98449b-0a56-4283-e2a5-5e595f7d4554",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Run below after copying the folder \"Project_learn2cut\" to your google drive\n",
    "\n",
    "#You will need to allow google drive to mount\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# from google.colab import files\n",
    "\n",
    "#IMPORTANT change below to \n",
    "#!cp -av /content/drive/<path>  /content/ \n",
    "#where <path> is the path to folder Project_learn2cut in your google drive. You can click on the folder icon on left and navigate to the path of this folder under drive/MyDrive to find the path.\n",
    "\n",
    "# !cp -av /content/drive/MyDrive/Colab\\ Notebooks/ORCS4529\\ Spring\\ 2022\\ public/Project_learn2cut/* /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6079,
     "status": "ok",
     "timestamp": 1669737550589,
     "user": {
      "displayName": "Shipra Agrawal",
      "userId": "12760872725468885671"
     },
     "user_tz": 300
    },
    "id": "xSXTKB2zurrt",
    "outputId": "0a8ed989-5fe1-48a5-f22e-1383ee8b7524",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install -i https://pypi.gurobi.com gurobipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13435,
     "status": "ok",
     "timestamp": 1669737564020,
     "user": {
      "displayName": "Shipra Agrawal",
      "userId": "12760872725468885671"
     },
     "user_tz": 300
    },
    "id": "YULy9ymNvDxN",
    "outputId": "dc0eb470-f94d-46a2-d25f-ed6bfb08b621",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install wandb -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 521855,
     "status": "ok",
     "timestamp": 1669738085857,
     "user": {
      "displayName": "Shipra Agrawal",
      "userId": "12760872725468885671"
     },
     "user_tz": 300
    },
    "id": "2xI0riE6md5V",
    "outputId": "0e4b479f-372f-46c2-dea6-4d1b2dd3e7c2",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/joaoromeuferraz/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjrferraz\u001b[0m (\u001b[33morcs4529\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/joaoromeuferraz/Documents/GitHub/Project_learn2cut/wandb/run-20221206_185831-3n3bgxg2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/orcs4529/finalproject/runs/3n3bgxg2\" target=\"_blank\">ruby-planet-878</a></strong> to <a href=\"https://wandb.ai/orcs4529/finalproject\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gymenv_v2\n",
    "from gymenv_v2 import make_multiple_env\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "run=wandb.init(project=\"finalproject\", entity=\"orcs4529\", tags=[\"training-easy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: You may generate your own instances on which you train the cutting agent.\n",
    "custom_config = {\n",
    "    \"load_dir\"        : 'instances/randomip_n60_m60',   # this is the location of the randomly generated instances (you may specify a different directory)\n",
    "    \"idx_list\"        : list(range(20)),                # take the first 20 instances from the directory\n",
    "    \"timelimit\"       : 50,                             # the maximum horizon length is 50\n",
    "    \"reward_type\"     : 'obj'                           # DO NOT CHANGE reward_type\n",
    "}\n",
    "\n",
    "# Easy Setup: Use the following environment settings. We will evaluate your agent with the same easy config below:\n",
    "easy_config = {\n",
    "    \"load_dir\"        : 'instances/train_10_n60_m60',\n",
    "    \"idx_list\"        : list(range(10)),\n",
    "    \"timelimit\"       : 50,\n",
    "    \"reward_type\"     : 'obj'\n",
    "}\n",
    "\n",
    "# Hard Setup: Use the following environment settings. We will evaluate your agent with the same hard config below:\n",
    "hard_config = {\n",
    "    \"load_dir\"        : 'instances/train_100_n60_m60',\n",
    "    \"idx_list\"        : list(range(99)),\n",
    "    \"timelimit\"       : 50,\n",
    "    \"reward_type\"     : 'obj'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training instances, dir instances/train_10_n60_m60 idx 0\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 1\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 2\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 3\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 4\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 5\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 6\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 7\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 8\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 9\n"
     ]
    }
   ],
   "source": [
    "env = make_multiple_env(**easy_config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = False\n",
    "t = 0 \n",
    "repisode = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(60, 60), (60,), (60,), (60, 60), (60,)]\n"
     ]
    }
   ],
   "source": [
    "print([s_.shape for s_ in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randint(0, s[-1].size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, r, d, _ = env.step(list(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(62, 60), (62,), (60,), (62, 60), (62,)]\n"
     ]
    }
   ],
   "source": [
    "print([s_.shape for s_ in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    # create env\n",
    " \n",
    "\n",
    "    for e in range(20):\n",
    "        # gym loop\n",
    "        s = env.reset()   # samples a random instance every time env.reset() is called\n",
    "        d = False\n",
    "        t = 0\n",
    "        repisode = 0\n",
    "\n",
    "        while not d:\n",
    "            #Take a random action\n",
    "            a = np.random.randint(0, s[-1].size, 1)            # s[-1].size shows the number of actions, i.e., cuts available at state s\n",
    "            \n",
    "            #simulate the environment to get the next state\n",
    "            s, r, d, _ = env.step(list(a))\n",
    "            print('episode', e, 'step', t, 'reward', r, 'action space size', s[-1].size, 'action', a[0])\n",
    "            \n",
    "            A, b, c0, cuts_a, cuts_b = s\n",
    "            #print(A.shape, b.shape, c0.shape, cuts_a.shape, cuts_b.shape)\n",
    "\n",
    "            t += 1\n",
    "            repisode += r\n",
    "\n",
    "    \t    #wandb logging\n",
    "            wandb.log({\"Training reward (easy config)\" : repisode})\n",
    "\t    #make sure to use the correct tag in wandb.init in the initialization on top\n",
    "\n",
    "\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
