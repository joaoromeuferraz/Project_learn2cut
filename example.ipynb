{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# IEOR4575 Project\n",
    "Instructor: Professor Shipra Agrawal\\\n",
    "Contributors: Yunhao Tang, Abhi Gupta\n",
    "\n",
    "## State-Action Description\n",
    "\n",
    "State s is an array with give components\n",
    "\n",
    "* s[0]:  constraint matrix $A$of the current LP ($\\max  -c^Tx \\text{ s.t. }Ax \\le  b$) . Dimension is $m \\times n$. See by printing s[0].shape. Here $n$ is the (fixed) number of variables. For instances of size 60 by 60 used in the above command, $n$ will remain fixed as 60. And $m$ is the current number of constraints. Initially, $m$ is to the number of constraints in the IP instance. (For instances generated with --num-c=60, $m$ is 60 at the first step).  But $m$ will increase by one in every step of the episode as one new constraint (cut) is added on taking an action.\n",
    "* s[1]: rhs $b$ for the current LP ($Ax\\le b$). Dimension same as the number $m$ in matrix A.\n",
    "* s[2]: coefficient vector $c$ from the LP objective ($-c^Tx$). Dimension same as the number of variables, i.e., $n$.\n",
    "* s[3],  s[4]: Gomory cuts available in the current round of Gomory's cutting plane algorithm. Each cut $i$ is of the form $D_i x\\le d_i$.   s[3] gives the matrix $D$ (of dimension $k \\times n$) of cuts and s[4] gives the rhs $d$ (of dimension $k$). The number of cuts $k$ available in each round changes, you can find it out by printing the size of last component of state, i.e., s[4].size or s[-1].size.\n",
    "\n",
    "## Example\n",
    "You can use the Jupyter notebook example.ipnyb on colab to familiarize yourself with the cutting plane environment that we have built for you.\n",
    "\n",
    "If you are using an offline environment (not colab) you can use example.py file.\n",
    "```\n",
    "$ python example.py\n",
    "```\n",
    "\n",
    "## TASK\n",
    "Train on two training environments: easy and hard:\n",
    " 10 instances and\n",
    "100 instances\n",
    "of size n=60, m=60, episode length 50\n",
    "\n",
    "Submit Code + Report of at most 5 pages, with algorithm, plots etc.\n",
    "Additional pages can be used to provide supplementary material which may or may not be reviewed, as necessary.\n",
    "\n",
    "These two can be loaded by using the following two configs (see example.py). Each mode is characterized by a set of parameters that define the cutting plane environment.\n",
    "\n",
    "The easy setup defines the environment as follows:\n",
    "```\n",
    "easy_config = {\n",
    "    \"load_dir\"        : 'instances/train_10_n60_m60',\n",
    "    \"idx_list\"        : list(range(10)),\n",
    "    \"timelimit\"       : 50,\n",
    "    \"reward_type\"     : 'obj'\n",
    "}\n",
    "```\n",
    "For your reference, the maximum total sum of rewards achievable in any given episode in the easy mode is 2.947 +- 0.5469.\n",
    "\n",
    "\n",
    "The hard setup defines the environment as follows:\n",
    "```\n",
    "hard_config = {\n",
    "    \"load_dir\"        : 'instances/train_100_n60_m60',\n",
    "    \"idx_list\"        : list(range(99)),\n",
    "    \"timelimit\"       : 50,\n",
    "    \"reward_type\"     : 'obj'\n",
    "}\n",
    "```\n",
    "On average, the maximum total sum of rewards achievable in any given episode in the hard mode is 2.985 +- 0.8427. But, the achieving close to 1 reward (i.e. closing the integrality gap by 1) is a reasonably good performance and can be achieved with what we have learned in this course.\n",
    "\n",
    "The main difference between the easy and hard modes is the number of training instances. Easy contains 10 instances while hard contains 100. Please read the ```example.py``` script would further details about what these environment parameters mean.\n",
    "\n",
    "## Generating New Instances (Optional)\n",
    "\n",
    "To make sure your algorithm generalizes to instances beyond those in the instances folder, you can create new environments with random IP instances and train/test on those. To generate new instances, run the following script. This will create 100 new instances with 60 constraints and 60 variables.\n",
    "\n",
    "You can show generalization performance on new instances that you didn't train for, for extra credit. You can also show other aspects of your solution like robustness to size of instances.\n",
    "\n",
    "```\n",
    "$ python generate_randomip.py --num-v 60 --num-c 60 --num-instances 100\n",
    "```\n",
    "\n",
    "The above instances will be saved in a directory named 'instances/randomip_n60_m60'. Then, we can load instances into gym env and train a cutting agent. The following code loads the 50th instance and run an episode with horizon 50:\n",
    "\n",
    "```\n",
    "python testgymenv.py --timelimit 50 --instance-idx 50 --instance-name randomip_n60_m60\n",
    "```\n",
    "\n",
    "We should see the printing of step information till the episode ends.\n",
    "\n",
    "If you do not provide --instance-idx, then the environment will load random instance out of the 100 instances in every episode. It is sometimes easier to train on a single instance to start with, instead of a pool of instances.\n",
    "\n",
    "## Notes\n",
    "\n",
    "- The env is not exactly equivalent to gym env where the state and action spaces are fixed. Here, the size of state and action space vary over time. The RL agent needs to handle variable state-action spaces.\n",
    "- The env uses python interface and computes optimal LP solution using Gurobi. If you are not using colab, make sure Gurobi is installed and license is valid. There is a free academic license as well as an online course limited use license available. See the installation instructions below. You don't need to do this if you are using example jupyter notebook in colab.\n",
    "\n",
    "## Installation\n",
    "```\n",
    "$ conda install -c gurobi gurobi\n",
    "```\n",
    "\n",
    "In addition, you need an academic license from gurobi. After getting the license, go to the license page.\n",
    "\n",
    "(https://www.gurobi.com/downloads/end-user-license-agreement-academic/)\n",
    "\n",
    " In order to activate the license, you will need to run the **grbgetkey** command with the license key written there. After this step, you can use the `ieor4575` environment that you have used for labs to complete the class project.\n",
    "\n",
    "## WandB for Visualizaition\n",
    "Class labs have made extensive use of wandb to familiarize you with some great machine learning visualization tools. You are encouraged to use wandb in the development of this project. See example notebook for the project name to use. You can move your best runs to the leaderboard.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## See README.md file for further details about the project and the environment.\n",
    "\n",
    "### State-Action Description\n",
    "\n",
    "### State\n",
    "State s is an array with give components\n",
    "\n",
    "* s[0]:  constraint matrix $A$of the current LP ($\\max  -c^Tx \\text{ s.t. }Ax \\le  b$) . Dimension is $m \\times n$. See by printing s[0].shape. Here $n$ is the (fixed) number of variables. For instances of size 60 by 60 used in the above command, $n$ will remain fixed as 60. And $m$ is the current number of constraints. Initially, $m$ is to the number of constraints in the IP instance. (For instances generated with --num-c=60, $m$ is 60 at the first step).  But $m$ will increase by one in every step of the episode as one new constraint (cut) is added on taking an action.\n",
    "* s[1]: rhs $b$ for the current LP ($Ax\\le b$). Dimension same as the number $m$ in matrix A.\n",
    "* s[2]: coefficient vector $c$ from the LP objective ($-c^Tx$). Dimension same as the number of variables, i.e., $n$.\n",
    "* s[3],  s[4]: Gomory cuts available in the current round of Gomory's cutting plane algorithm. Each cut $i$ is of the form $D_i x\\le d_i$.   s[3] gives the matrix $D$ (of dimension $k \\times n$) of cuts and s[4] gives the rhs $d$ (of dimension $k$). The number of cuts $k$ available in each round changes, you can find it out by printing the size of last component of state, i.e., s[4].size or s[-1].size.\n",
    "\n",
    "### Actions\n",
    "There are k=s[4].size actions available in each state $s$, with $i^{th}$ action corresponding to the $i^{th}$ cut with inequality $D_i x\\le d_i$ in $s[3], s[4]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33757,
     "status": "ok",
     "timestamp": 1669737544513,
     "user": {
      "displayName": "Shipra Agrawal",
      "userId": "12760872725468885671"
     },
     "user_tz": 300
    },
    "id": "1H51OwGYq2Tp",
    "outputId": "9b98449b-0a56-4283-e2a5-5e595f7d4554",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/example.ipynb' -> '/content/example.ipynb'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/example.py' -> '/content/example.py'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/findgaps.py' -> '/content/findgaps.py'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/generate_randomip.py' -> '/content/generate_randomip.py'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/gurobiutils.py' -> '/content/gurobiutils.py'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/gymenv.py' -> '/content/gymenv.py'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/gymenv_v2.py' -> '/content/gymenv_v2.py'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances' -> '/content/instances'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60' -> '/content/instances/train_100_n60_m60'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_28.npy' -> '/content/instances/train_100_n60_m60/A_28.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_1.npy' -> '/content/instances/train_100_n60_m60/A_1.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_17.npy' -> '/content/instances/train_100_n60_m60/A_17.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_3.npy' -> '/content/instances/train_100_n60_m60/A_3.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_10.npy' -> '/content/instances/train_100_n60_m60/A_10.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_20.npy' -> '/content/instances/train_100_n60_m60/A_20.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_34.npy' -> '/content/instances/train_100_n60_m60/A_34.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_14.npy' -> '/content/instances/train_100_n60_m60/A_14.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_2.npy' -> '/content/instances/train_100_n60_m60/A_2.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_21.npy' -> '/content/instances/train_100_n60_m60/A_21.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_15.npy' -> '/content/instances/train_100_n60_m60/A_15.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_32.npy' -> '/content/instances/train_100_n60_m60/A_32.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_11.npy' -> '/content/instances/train_100_n60_m60/A_11.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_38.npy' -> '/content/instances/train_100_n60_m60/A_38.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_36.npy' -> '/content/instances/train_100_n60_m60/A_36.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_25.npy' -> '/content/instances/train_100_n60_m60/A_25.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_16.npy' -> '/content/instances/train_100_n60_m60/A_16.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_30.npy' -> '/content/instances/train_100_n60_m60/A_30.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_22.npy' -> '/content/instances/train_100_n60_m60/A_22.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_27.npy' -> '/content/instances/train_100_n60_m60/A_27.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_0.npy' -> '/content/instances/train_100_n60_m60/A_0.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_33.npy' -> '/content/instances/train_100_n60_m60/A_33.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_18.npy' -> '/content/instances/train_100_n60_m60/A_18.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_37.npy' -> '/content/instances/train_100_n60_m60/A_37.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_12.npy' -> '/content/instances/train_100_n60_m60/A_12.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_35.npy' -> '/content/instances/train_100_n60_m60/A_35.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_23.npy' -> '/content/instances/train_100_n60_m60/A_23.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_31.npy' -> '/content/instances/train_100_n60_m60/A_31.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_29.npy' -> '/content/instances/train_100_n60_m60/A_29.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_26.npy' -> '/content/instances/train_100_n60_m60/A_26.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_19.npy' -> '/content/instances/train_100_n60_m60/A_19.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_13.npy' -> '/content/instances/train_100_n60_m60/A_13.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_24.npy' -> '/content/instances/train_100_n60_m60/A_24.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_6.npy' -> '/content/instances/train_100_n60_m60/A_6.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_53.npy' -> '/content/instances/train_100_n60_m60/A_53.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_72.npy' -> '/content/instances/train_100_n60_m60/A_72.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_42.npy' -> '/content/instances/train_100_n60_m60/A_42.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_67.npy' -> '/content/instances/train_100_n60_m60/A_67.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_71.npy' -> '/content/instances/train_100_n60_m60/A_71.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_61.npy' -> '/content/instances/train_100_n60_m60/A_61.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_46.npy' -> '/content/instances/train_100_n60_m60/A_46.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_8.npy' -> '/content/instances/train_100_n60_m60/A_8.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_66.npy' -> '/content/instances/train_100_n60_m60/A_66.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_50.npy' -> '/content/instances/train_100_n60_m60/A_50.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_43.npy' -> '/content/instances/train_100_n60_m60/A_43.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_70.npy' -> '/content/instances/train_100_n60_m60/A_70.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_5.npy' -> '/content/instances/train_100_n60_m60/A_5.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_58.npy' -> '/content/instances/train_100_n60_m60/A_58.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_78.npy' -> '/content/instances/train_100_n60_m60/A_78.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_54.npy' -> '/content/instances/train_100_n60_m60/A_54.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_60.npy' -> '/content/instances/train_100_n60_m60/A_60.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_62.npy' -> '/content/instances/train_100_n60_m60/A_62.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_64.npy' -> '/content/instances/train_100_n60_m60/A_64.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_55.npy' -> '/content/instances/train_100_n60_m60/A_55.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_47.npy' -> '/content/instances/train_100_n60_m60/A_47.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_49.npy' -> '/content/instances/train_100_n60_m60/A_49.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_56.npy' -> '/content/instances/train_100_n60_m60/A_56.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_7.npy' -> '/content/instances/train_100_n60_m60/A_7.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_57.npy' -> '/content/instances/train_100_n60_m60/A_57.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_73.npy' -> '/content/instances/train_100_n60_m60/A_73.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_39.npy' -> '/content/instances/train_100_n60_m60/A_39.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_76.npy' -> '/content/instances/train_100_n60_m60/A_76.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_40.npy' -> '/content/instances/train_100_n60_m60/A_40.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_45.npy' -> '/content/instances/train_100_n60_m60/A_45.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_75.npy' -> '/content/instances/train_100_n60_m60/A_75.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_77.npy' -> '/content/instances/train_100_n60_m60/A_77.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_4.npy' -> '/content/instances/train_100_n60_m60/A_4.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_69.npy' -> '/content/instances/train_100_n60_m60/A_69.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_51.npy' -> '/content/instances/train_100_n60_m60/A_51.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_68.npy' -> '/content/instances/train_100_n60_m60/A_68.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_41.npy' -> '/content/instances/train_100_n60_m60/A_41.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_44.npy' -> '/content/instances/train_100_n60_m60/A_44.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_65.npy' -> '/content/instances/train_100_n60_m60/A_65.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_59.npy' -> '/content/instances/train_100_n60_m60/A_59.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_74.npy' -> '/content/instances/train_100_n60_m60/A_74.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_79.npy' -> '/content/instances/train_100_n60_m60/A_79.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_63.npy' -> '/content/instances/train_100_n60_m60/A_63.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_48.npy' -> '/content/instances/train_100_n60_m60/A_48.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_52.npy' -> '/content/instances/train_100_n60_m60/A_52.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_23.npy' -> '/content/instances/train_100_n60_m60/b_23.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_87.npy' -> '/content/instances/train_100_n60_m60/A_87.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_84.npy' -> '/content/instances/train_100_n60_m60/A_84.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_97.npy' -> '/content/instances/train_100_n60_m60/A_97.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_22.npy' -> '/content/instances/train_100_n60_m60/b_22.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_85.npy' -> '/content/instances/train_100_n60_m60/A_85.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_15.npy' -> '/content/instances/train_100_n60_m60/b_15.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_30.npy' -> '/content/instances/train_100_n60_m60/b_30.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_88.npy' -> '/content/instances/train_100_n60_m60/A_88.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_40.npy' -> '/content/instances/train_100_n60_m60/b_40.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_81.npy' -> '/content/instances/train_100_n60_m60/A_81.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_21.npy' -> '/content/instances/train_100_n60_m60/b_21.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_12.npy' -> '/content/instances/train_100_n60_m60/b_12.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_4.npy' -> '/content/instances/train_100_n60_m60/b_4.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_35.npy' -> '/content/instances/train_100_n60_m60/b_35.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_93.npy' -> '/content/instances/train_100_n60_m60/A_93.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_89.npy' -> '/content/instances/train_100_n60_m60/A_89.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_26.npy' -> '/content/instances/train_100_n60_m60/b_26.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_82.npy' -> '/content/instances/train_100_n60_m60/A_82.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_83.npy' -> '/content/instances/train_100_n60_m60/A_83.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_14.npy' -> '/content/instances/train_100_n60_m60/b_14.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_19.npy' -> '/content/instances/train_100_n60_m60/b_19.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_38.npy' -> '/content/instances/train_100_n60_m60/b_38.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_24.npy' -> '/content/instances/train_100_n60_m60/b_24.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_10.npy' -> '/content/instances/train_100_n60_m60/b_10.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_31.npy' -> '/content/instances/train_100_n60_m60/b_31.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_20.npy' -> '/content/instances/train_100_n60_m60/b_20.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_34.npy' -> '/content/instances/train_100_n60_m60/b_34.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_1.npy' -> '/content/instances/train_100_n60_m60/b_1.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_41.npy' -> '/content/instances/train_100_n60_m60/b_41.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_91.npy' -> '/content/instances/train_100_n60_m60/A_91.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_92.npy' -> '/content/instances/train_100_n60_m60/A_92.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_13.npy' -> '/content/instances/train_100_n60_m60/b_13.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_96.npy' -> '/content/instances/train_100_n60_m60/A_96.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_25.npy' -> '/content/instances/train_100_n60_m60/b_25.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_86.npy' -> '/content/instances/train_100_n60_m60/A_86.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_27.npy' -> '/content/instances/train_100_n60_m60/b_27.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_80.npy' -> '/content/instances/train_100_n60_m60/A_80.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_32.npy' -> '/content/instances/train_100_n60_m60/b_32.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_28.npy' -> '/content/instances/train_100_n60_m60/b_28.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_18.npy' -> '/content/instances/train_100_n60_m60/b_18.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_98.npy' -> '/content/instances/train_100_n60_m60/A_98.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_36.npy' -> '/content/instances/train_100_n60_m60/b_36.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_39.npy' -> '/content/instances/train_100_n60_m60/b_39.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_37.npy' -> '/content/instances/train_100_n60_m60/b_37.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_33.npy' -> '/content/instances/train_100_n60_m60/b_33.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_94.npy' -> '/content/instances/train_100_n60_m60/A_94.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_2.npy' -> '/content/instances/train_100_n60_m60/b_2.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_17.npy' -> '/content/instances/train_100_n60_m60/b_17.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_9.npy' -> '/content/instances/train_100_n60_m60/A_9.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_29.npy' -> '/content/instances/train_100_n60_m60/b_29.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_16.npy' -> '/content/instances/train_100_n60_m60/b_16.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_42.npy' -> '/content/instances/train_100_n60_m60/b_42.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_90.npy' -> '/content/instances/train_100_n60_m60/A_90.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_3.npy' -> '/content/instances/train_100_n60_m60/b_3.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_11.npy' -> '/content/instances/train_100_n60_m60/b_11.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/A_95.npy' -> '/content/instances/train_100_n60_m60/A_95.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_0.npy' -> '/content/instances/train_100_n60_m60/b_0.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_66.npy' -> '/content/instances/train_100_n60_m60/b_66.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_65.npy' -> '/content/instances/train_100_n60_m60/b_65.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_49.npy' -> '/content/instances/train_100_n60_m60/b_49.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_79.npy' -> '/content/instances/train_100_n60_m60/b_79.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_0.npy' -> '/content/instances/train_100_n60_m60/c_0.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_80.npy' -> '/content/instances/train_100_n60_m60/b_80.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_97.npy' -> '/content/instances/train_100_n60_m60/b_97.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_51.npy' -> '/content/instances/train_100_n60_m60/b_51.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_83.npy' -> '/content/instances/train_100_n60_m60/b_83.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_50.npy' -> '/content/instances/train_100_n60_m60/b_50.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_11.npy' -> '/content/instances/train_100_n60_m60/c_11.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_10.npy' -> '/content/instances/train_100_n60_m60/c_10.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_58.npy' -> '/content/instances/train_100_n60_m60/b_58.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_93.npy' -> '/content/instances/train_100_n60_m60/b_93.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_94.npy' -> '/content/instances/train_100_n60_m60/b_94.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_5.npy' -> '/content/instances/train_100_n60_m60/b_5.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_95.npy' -> '/content/instances/train_100_n60_m60/b_95.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_6.npy' -> '/content/instances/train_100_n60_m60/b_6.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_96.npy' -> '/content/instances/train_100_n60_m60/b_96.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_57.npy' -> '/content/instances/train_100_n60_m60/b_57.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_85.npy' -> '/content/instances/train_100_n60_m60/b_85.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_98.npy' -> '/content/instances/train_100_n60_m60/b_98.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_9.npy' -> '/content/instances/train_100_n60_m60/b_9.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_68.npy' -> '/content/instances/train_100_n60_m60/b_68.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_89.npy' -> '/content/instances/train_100_n60_m60/b_89.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_53.npy' -> '/content/instances/train_100_n60_m60/b_53.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_55.npy' -> '/content/instances/train_100_n60_m60/b_55.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_71.npy' -> '/content/instances/train_100_n60_m60/b_71.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_45.npy' -> '/content/instances/train_100_n60_m60/b_45.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_44.npy' -> '/content/instances/train_100_n60_m60/b_44.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_64.npy' -> '/content/instances/train_100_n60_m60/b_64.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_48.npy' -> '/content/instances/train_100_n60_m60/b_48.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_70.npy' -> '/content/instances/train_100_n60_m60/b_70.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_52.npy' -> '/content/instances/train_100_n60_m60/b_52.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_54.npy' -> '/content/instances/train_100_n60_m60/b_54.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_88.npy' -> '/content/instances/train_100_n60_m60/b_88.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_90.npy' -> '/content/instances/train_100_n60_m60/b_90.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_87.npy' -> '/content/instances/train_100_n60_m60/b_87.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_74.npy' -> '/content/instances/train_100_n60_m60/b_74.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_46.npy' -> '/content/instances/train_100_n60_m60/b_46.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_73.npy' -> '/content/instances/train_100_n60_m60/b_73.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_92.npy' -> '/content/instances/train_100_n60_m60/b_92.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_62.npy' -> '/content/instances/train_100_n60_m60/b_62.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_82.npy' -> '/content/instances/train_100_n60_m60/b_82.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_78.npy' -> '/content/instances/train_100_n60_m60/b_78.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_60.npy' -> '/content/instances/train_100_n60_m60/b_60.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_72.npy' -> '/content/instances/train_100_n60_m60/b_72.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_47.npy' -> '/content/instances/train_100_n60_m60/b_47.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_84.npy' -> '/content/instances/train_100_n60_m60/b_84.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_63.npy' -> '/content/instances/train_100_n60_m60/b_63.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_76.npy' -> '/content/instances/train_100_n60_m60/b_76.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_1.npy' -> '/content/instances/train_100_n60_m60/c_1.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_86.npy' -> '/content/instances/train_100_n60_m60/b_86.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_7.npy' -> '/content/instances/train_100_n60_m60/b_7.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_91.npy' -> '/content/instances/train_100_n60_m60/b_91.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_59.npy' -> '/content/instances/train_100_n60_m60/b_59.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_43.npy' -> '/content/instances/train_100_n60_m60/b_43.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_67.npy' -> '/content/instances/train_100_n60_m60/b_67.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_56.npy' -> '/content/instances/train_100_n60_m60/b_56.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_81.npy' -> '/content/instances/train_100_n60_m60/b_81.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_69.npy' -> '/content/instances/train_100_n60_m60/b_69.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_77.npy' -> '/content/instances/train_100_n60_m60/b_77.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_75.npy' -> '/content/instances/train_100_n60_m60/b_75.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_8.npy' -> '/content/instances/train_100_n60_m60/b_8.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/b_61.npy' -> '/content/instances/train_100_n60_m60/b_61.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_17.npy' -> '/content/instances/train_100_n60_m60/c_17.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_39.npy' -> '/content/instances/train_100_n60_m60/c_39.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_61.npy' -> '/content/instances/train_100_n60_m60/c_61.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_52.npy' -> '/content/instances/train_100_n60_m60/c_52.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_22.npy' -> '/content/instances/train_100_n60_m60/c_22.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_26.npy' -> '/content/instances/train_100_n60_m60/c_26.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_70.npy' -> '/content/instances/train_100_n60_m60/c_70.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_54.npy' -> '/content/instances/train_100_n60_m60/c_54.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_2.npy' -> '/content/instances/train_100_n60_m60/c_2.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_21.npy' -> '/content/instances/train_100_n60_m60/c_21.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_20.npy' -> '/content/instances/train_100_n60_m60/c_20.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_23.npy' -> '/content/instances/train_100_n60_m60/c_23.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_42.npy' -> '/content/instances/train_100_n60_m60/c_42.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_40.npy' -> '/content/instances/train_100_n60_m60/c_40.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_60.npy' -> '/content/instances/train_100_n60_m60/c_60.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_68.npy' -> '/content/instances/train_100_n60_m60/c_68.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_48.npy' -> '/content/instances/train_100_n60_m60/c_48.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_44.npy' -> '/content/instances/train_100_n60_m60/c_44.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_13.npy' -> '/content/instances/train_100_n60_m60/c_13.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_67.npy' -> '/content/instances/train_100_n60_m60/c_67.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_43.npy' -> '/content/instances/train_100_n60_m60/c_43.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_30.npy' -> '/content/instances/train_100_n60_m60/c_30.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_25.npy' -> '/content/instances/train_100_n60_m60/c_25.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_56.npy' -> '/content/instances/train_100_n60_m60/c_56.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_12.npy' -> '/content/instances/train_100_n60_m60/c_12.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_69.npy' -> '/content/instances/train_100_n60_m60/c_69.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_49.npy' -> '/content/instances/train_100_n60_m60/c_49.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_28.npy' -> '/content/instances/train_100_n60_m60/c_28.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_50.npy' -> '/content/instances/train_100_n60_m60/c_50.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_51.npy' -> '/content/instances/train_100_n60_m60/c_51.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_24.npy' -> '/content/instances/train_100_n60_m60/c_24.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_71.npy' -> '/content/instances/train_100_n60_m60/c_71.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_55.npy' -> '/content/instances/train_100_n60_m60/c_55.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_15.npy' -> '/content/instances/train_100_n60_m60/c_15.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_27.npy' -> '/content/instances/train_100_n60_m60/c_27.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_41.npy' -> '/content/instances/train_100_n60_m60/c_41.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_64.npy' -> '/content/instances/train_100_n60_m60/c_64.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_63.npy' -> '/content/instances/train_100_n60_m60/c_63.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_14.npy' -> '/content/instances/train_100_n60_m60/c_14.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_16.npy' -> '/content/instances/train_100_n60_m60/c_16.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_37.npy' -> '/content/instances/train_100_n60_m60/c_37.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_66.npy' -> '/content/instances/train_100_n60_m60/c_66.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_3.npy' -> '/content/instances/train_100_n60_m60/c_3.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_57.npy' -> '/content/instances/train_100_n60_m60/c_57.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_58.npy' -> '/content/instances/train_100_n60_m60/c_58.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_31.npy' -> '/content/instances/train_100_n60_m60/c_31.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_65.npy' -> '/content/instances/train_100_n60_m60/c_65.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_18.npy' -> '/content/instances/train_100_n60_m60/c_18.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_32.npy' -> '/content/instances/train_100_n60_m60/c_32.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_33.npy' -> '/content/instances/train_100_n60_m60/c_33.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_29.npy' -> '/content/instances/train_100_n60_m60/c_29.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_19.npy' -> '/content/instances/train_100_n60_m60/c_19.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_7.npy' -> '/content/instances/train_100_n60_m60/c_7.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_38.npy' -> '/content/instances/train_100_n60_m60/c_38.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_4.npy' -> '/content/instances/train_100_n60_m60/c_4.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_34.npy' -> '/content/instances/train_100_n60_m60/c_34.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_53.npy' -> '/content/instances/train_100_n60_m60/c_53.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_35.npy' -> '/content/instances/train_100_n60_m60/c_35.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_47.npy' -> '/content/instances/train_100_n60_m60/c_47.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_62.npy' -> '/content/instances/train_100_n60_m60/c_62.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_46.npy' -> '/content/instances/train_100_n60_m60/c_46.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_59.npy' -> '/content/instances/train_100_n60_m60/c_59.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_45.npy' -> '/content/instances/train_100_n60_m60/c_45.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_36.npy' -> '/content/instances/train_100_n60_m60/c_36.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_6.npy' -> '/content/instances/train_100_n60_m60/c_6.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_5.npy' -> '/content/instances/train_100_n60_m60/c_5.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_83.npy' -> '/content/instances/train_100_n60_m60/c_83.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_94.npy' -> '/content/instances/train_100_n60_m60/c_94.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_81.npy' -> '/content/instances/train_100_n60_m60/c_81.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_80.npy' -> '/content/instances/train_100_n60_m60/c_80.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_79.npy' -> '/content/instances/train_100_n60_m60/c_79.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_97.npy' -> '/content/instances/train_100_n60_m60/c_97.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_75.npy' -> '/content/instances/train_100_n60_m60/c_75.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_9.npy' -> '/content/instances/train_100_n60_m60/c_9.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_73.npy' -> '/content/instances/train_100_n60_m60/c_73.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_90.npy' -> '/content/instances/train_100_n60_m60/c_90.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_8.npy' -> '/content/instances/train_100_n60_m60/c_8.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_84.npy' -> '/content/instances/train_100_n60_m60/c_84.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_85.npy' -> '/content/instances/train_100_n60_m60/c_85.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_72.npy' -> '/content/instances/train_100_n60_m60/c_72.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_74.npy' -> '/content/instances/train_100_n60_m60/c_74.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_91.npy' -> '/content/instances/train_100_n60_m60/c_91.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_87.npy' -> '/content/instances/train_100_n60_m60/c_87.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_88.npy' -> '/content/instances/train_100_n60_m60/c_88.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_82.npy' -> '/content/instances/train_100_n60_m60/c_82.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_89.npy' -> '/content/instances/train_100_n60_m60/c_89.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_92.npy' -> '/content/instances/train_100_n60_m60/c_92.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_95.npy' -> '/content/instances/train_100_n60_m60/c_95.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_98.npy' -> '/content/instances/train_100_n60_m60/c_98.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_96.npy' -> '/content/instances/train_100_n60_m60/c_96.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_76.npy' -> '/content/instances/train_100_n60_m60/c_76.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_93.npy' -> '/content/instances/train_100_n60_m60/c_93.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_86.npy' -> '/content/instances/train_100_n60_m60/c_86.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_78.npy' -> '/content/instances/train_100_n60_m60/c_78.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_100_n60_m60/c_77.npy' -> '/content/instances/train_100_n60_m60/c_77.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_10_n60_m60' -> '/content/instances/train_10_n60_m60'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_10_n60_m60/c_0.npy' -> '/content/instances/train_10_n60_m60/c_0.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_10_n60_m60/c_5.npy' -> '/content/instances/train_10_n60_m60/c_5.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_10_n60_m60/A_7.npy' -> '/content/instances/train_10_n60_m60/A_7.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_10_n60_m60/c_3.npy' -> '/content/instances/train_10_n60_m60/c_3.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_10_n60_m60/b_8.npy' -> '/content/instances/train_10_n60_m60/b_8.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_10_n60_m60/A_2.npy' -> '/content/instances/train_10_n60_m60/A_2.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_10_n60_m60/c_8.npy' -> '/content/instances/train_10_n60_m60/c_8.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_10_n60_m60/c_2.npy' -> '/content/instances/train_10_n60_m60/c_2.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_10_n60_m60/c_7.npy' -> '/content/instances/train_10_n60_m60/c_7.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_10_n60_m60/A_9.npy' -> '/content/instances/train_10_n60_m60/A_9.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_10_n60_m60/b_9.npy' -> '/content/instances/train_10_n60_m60/b_9.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_10_n60_m60/b_4.npy' -> '/content/instances/train_10_n60_m60/b_4.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_10_n60_m60/b_0.npy' -> '/content/instances/train_10_n60_m60/b_0.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_10_n60_m60/A_5.npy' -> '/content/instances/train_10_n60_m60/A_5.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_10_n60_m60/A_0.npy' -> '/content/instances/train_10_n60_m60/A_0.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_10_n60_m60/b_2.npy' -> '/content/instances/train_10_n60_m60/b_2.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_10_n60_m60/b_1.npy' -> '/content/instances/train_10_n60_m60/b_1.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_10_n60_m60/A_3.npy' -> '/content/instances/train_10_n60_m60/A_3.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_10_n60_m60/b_6.npy' -> '/content/instances/train_10_n60_m60/b_6.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_10_n60_m60/c_4.npy' -> '/content/instances/train_10_n60_m60/c_4.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_10_n60_m60/b_7.npy' -> '/content/instances/train_10_n60_m60/b_7.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_10_n60_m60/c_6.npy' -> '/content/instances/train_10_n60_m60/c_6.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_10_n60_m60/c_9.npy' -> '/content/instances/train_10_n60_m60/c_9.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_10_n60_m60/A_4.npy' -> '/content/instances/train_10_n60_m60/A_4.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_10_n60_m60/b_3.npy' -> '/content/instances/train_10_n60_m60/b_3.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_10_n60_m60/c_1.npy' -> '/content/instances/train_10_n60_m60/c_1.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_10_n60_m60/A_6.npy' -> '/content/instances/train_10_n60_m60/A_6.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_10_n60_m60/A_8.npy' -> '/content/instances/train_10_n60_m60/A_8.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_10_n60_m60/b_5.npy' -> '/content/instances/train_10_n60_m60/b_5.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/instances/train_10_n60_m60/A_1.npy' -> '/content/instances/train_10_n60_m60/A_1.npy'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/README.md' -> '/content/README.md'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/solverutils.py' -> '/content/solverutils.py'\n",
      "'/content/drive/MyDrive/Colab Notebooks/ORCS4529 Spring 2022 public/Project_learn2cut/testgymenv.py' -> '/content/testgymenv.py'\n"
     ]
    }
   ],
   "source": [
    "#Run below after copying the folder \"Project_learn2cut\" to your google drive\n",
    "\n",
    "#You will need to allow google drive to mount\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "from google.colab import files\n",
    "\n",
    "#IMPORTANT change below to \n",
    "#!cp -av /content/drive/<path>  /content/ \n",
    "#where <path> is the path to folder Project_learn2cut in your google drive. You can click on the folder icon on left and navigate to the path of this folder under drive/MyDrive to find the path.\n",
    "\n",
    "!cp -av /content/drive/MyDrive/Colab\\ Notebooks/ORCS4529\\ Spring\\ 2022\\ public/Project_learn2cut/* /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6079,
     "status": "ok",
     "timestamp": 1669737550589,
     "user": {
      "displayName": "Shipra Agrawal",
      "userId": "12760872725468885671"
     },
     "user_tz": 300
    },
    "id": "xSXTKB2zurrt",
    "outputId": "0a8ed989-5fe1-48a5-f22e-1383ee8b7524",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.gurobi.com, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting gurobipy\n",
      "  Downloading gurobipy-10.0.0-cp37-cp37m-manylinux2014_x86_64.whl (12.9 MB)\n",
      "\u001b[K     || 12.9 MB 5.2 MB/s \n",
      "\u001b[?25hInstalling collected packages: gurobipy\n",
      "Successfully installed gurobipy-10.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -i https://pypi.gurobi.com gurobipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13435,
     "status": "ok",
     "timestamp": 1669737564020,
     "user": {
      "displayName": "Shipra Agrawal",
      "userId": "12760872725468885671"
     },
     "user_tz": 300
    },
    "id": "YULy9ymNvDxN",
    "outputId": "dc0eb470-f94d-46a2-d25f-ed6bfb08b621",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     || 1.9 MB 5.0 MB/s \n",
      "\u001b[K     || 168 kB 34.2 MB/s \n",
      "\u001b[K     || 182 kB 55.8 MB/s \n",
      "\u001b[K     || 62 kB 1.0 MB/s \n",
      "\u001b[K     || 168 kB 55.0 MB/s \n",
      "\u001b[K     || 166 kB 54.4 MB/s \n",
      "\u001b[K     || 166 kB 53.7 MB/s \n",
      "\u001b[K     || 162 kB 49.2 MB/s \n",
      "\u001b[K     || 162 kB 56.0 MB/s \n",
      "\u001b[K     || 158 kB 46.2 MB/s \n",
      "\u001b[K     || 157 kB 45.2 MB/s \n",
      "\u001b[K     || 157 kB 55.6 MB/s \n",
      "\u001b[K     || 157 kB 58.8 MB/s \n",
      "\u001b[K     || 157 kB 57.1 MB/s \n",
      "\u001b[K     || 157 kB 38.0 MB/s \n",
      "\u001b[K     || 157 kB 44.9 MB/s \n",
      "\u001b[K     || 157 kB 47.2 MB/s \n",
      "\u001b[K     || 156 kB 45.3 MB/s \n",
      "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 521855,
     "status": "ok",
     "timestamp": 1669738085857,
     "user": {
      "displayName": "Shipra Agrawal",
      "userId": "12760872725468885671"
     },
     "user_tz": 300
    },
    "id": "2xI0riE6md5V",
    "outputId": "0e4b479f-372f-46c2-dea6-4d1b2dd3e7c2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33magrawals\u001b[0m (\u001b[33morcs4529\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20221129_155929-3muel36b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/orcs4529/finalproject/runs/3muel36b\" target=\"_blank\">frosty-dragon-119</a></strong> to <a href=\"https://wandb.ai/orcs4529/finalproject\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training instances, dir instances/train_10_n60_m60 idx 0\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 1\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 2\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 3\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 4\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 5\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 6\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 7\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 8\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 9\n",
      "Restricted license - for non-production use only - expires 2024-10-28\n",
      "episode 0 step 0 reward 0.006921637548430226 action space size 61 action 15\n",
      "episode 0 step 1 reward 0.010864387055107727 action space size 62 action 42\n",
      "episode 0 step 2 reward 0.0030268555049133283 action space size 60 action 44\n",
      "episode 0 step 3 reward 0.007195865762696485 action space size 65 action 50\n",
      "episode 0 step 4 reward 9.454302426092909e-05 action space size 65 action 14\n",
      "episode 0 step 5 reward 0.002932128235215714 action space size 66 action 25\n",
      "episode 0 step 6 reward 0.0016016483448311192 action space size 66 action 59\n",
      "episode 0 step 7 reward 0.008454020433418918 action space size 68 action 29\n",
      "episode 0 step 8 reward 0.002941182539871079 action space size 68 action 53\n",
      "episode 0 step 9 reward 1.9044023247261066e-06 action space size 69 action 23\n",
      "episode 0 step 10 reward 3.681924249576696e-05 action space size 72 action 35\n",
      "episode 0 step 11 reward 0.00022121224242255266 action space size 73 action 15\n",
      "episode 0 step 12 reward 3.154668888782908e-05 action space size 69 action 16\n",
      "episode 0 step 13 reward 0.0007418625211812468 action space size 73 action 22\n",
      "episode 0 step 14 reward 0.0009607772879007825 action space size 76 action 10\n",
      "episode 0 step 15 reward 0.00045430647264765867 action space size 76 action 37\n",
      "episode 0 step 16 reward 0.0035625449240797025 action space size 74 action 5\n",
      "episode 0 step 17 reward 0.0022049400845389755 action space size 75 action 21\n",
      "episode 0 step 18 reward 0.0014693266930407844 action space size 77 action 43\n",
      "episode 0 step 19 reward 0.00011851473914248345 action space size 77 action 14\n",
      "episode 0 step 20 reward 4.5047264165987144e-05 action space size 77 action 56\n",
      "episode 0 step 21 reward 0.00037543694384112314 action space size 78 action 6\n",
      "episode 0 step 22 reward 0.0017688235593595891 action space size 84 action 9\n",
      "episode 0 step 23 reward 1.3404887795331888e-06 action space size 84 action 46\n",
      "episode 0 step 24 reward 0.0004184429299129988 action space size 84 action 37\n",
      "episode 0 step 25 reward 0.000613779919603985 action space size 87 action 73\n",
      "episode 0 step 26 reward 0.00020142457674410252 action space size 86 action 74\n",
      "episode 0 step 27 reward 0.000140158826525294 action space size 86 action 11\n",
      "episode 0 step 28 reward 0.0004033321301903925 action space size 88 action 3\n",
      "episode 0 step 29 reward 0.0006247315418477228 action space size 88 action 66\n",
      "episode 0 step 30 reward 0.0004171768355263339 action space size 90 action 28\n",
      "episode 0 step 31 reward 0.0007596127256874752 action space size 92 action 29\n",
      "episode 0 step 32 reward 2.9411888363028993e-05 action space size 94 action 70\n",
      "episode 0 step 33 reward 0.0012786875486199278 action space size 94 action 24\n",
      "episode 0 step 34 reward 3.347206552462012e-05 action space size 94 action 42\n",
      "episode 0 step 35 reward 0.00011334925329720136 action space size 94 action 18\n",
      "episode 0 step 36 reward 0.0002880997531065077 action space size 96 action 13\n",
      "episode 0 step 37 reward 5.51117739178153e-05 action space size 94 action 73\n",
      "episode 0 step 38 reward 9.971683766707429e-06 action space size 97 action 86\n",
      "episode 0 step 39 reward 4.793665721081197e-05 action space size 99 action 14\n",
      "episode 0 step 40 reward 0.00010097720496560214 action space size 101 action 85\n",
      "episode 0 step 41 reward 7.041793423923082e-05 action space size 98 action 42\n",
      "episode 0 step 42 reward 2.9456379252223996e-05 action space size 97 action 23\n",
      "episode 0 step 43 reward 0.00027016412514058175 action space size 102 action 19\n",
      "episode 0 step 44 reward 0.00019689410783030326 action space size 103 action 57\n",
      "episode 0 step 45 reward 0.0001678036414887174 action space size 105 action 39\n",
      "episode 0 step 46 reward 0.0005944106210336031 action space size 106 action 47\n",
      "episode 0 step 47 reward 3.752166480808228e-05 action space size 108 action 26\n",
      "episode 0 step 48 reward 8.706993457963108e-05 action space size 108 action 8\n",
      "episode 0 step 49 reward 0.00011847802738884639 action space size 110 action 33\n",
      "episode 1 step 0 reward 0.017462432909724157 action space size 62 action 11\n",
      "episode 1 step 1 reward 0.5977127340108837 action space size 58 action 0\n",
      "episode 1 step 2 reward 4.547473508864641e-13 action space size 63 action 52\n",
      "episode 1 step 3 reward 0.0 action space size 64 action 5\n",
      "episode 1 step 4 reward 2.2737367544323206e-13 action space size 63 action 12\n",
      "episode 1 step 5 reward 1.3642420526593924e-12 action space size 64 action 50\n",
      "episode 1 step 6 reward 9.094947017729282e-13 action space size 66 action 31\n",
      "episode 1 step 7 reward 2.2737367544323206e-13 action space size 67 action 39\n",
      "episode 1 step 8 reward 2.2737367544323206e-13 action space size 65 action 38\n",
      "episode 1 step 9 reward 2.2737367544323206e-13 action space size 70 action 27\n",
      "episode 1 step 10 reward 1.3642420526593924e-12 action space size 70 action 13\n",
      "episode 1 step 11 reward 1.1368683772161603e-12 action space size 69 action 1\n",
      "episode 1 step 12 reward 4.547473508864641e-13 action space size 71 action 56\n",
      "episode 1 step 13 reward 4.547473508864641e-13 action space size 69 action 30\n",
      "episode 1 step 14 reward 0.0 action space size 73 action 44\n",
      "episode 1 step 15 reward 2.2737367544323206e-13 action space size 73 action 18\n",
      "episode 1 step 16 reward 0.0 action space size 74 action 61\n",
      "episode 1 step 17 reward 4.547473508864641e-13 action space size 78 action 73\n",
      "episode 1 step 18 reward 4.547473508864641e-13 action space size 76 action 58\n",
      "episode 1 step 19 reward 2.2737367544323206e-13 action space size 80 action 63\n",
      "episode 1 step 20 reward 2.2737367544323206e-13 action space size 80 action 35\n",
      "episode 1 step 21 reward 4.547473508864641e-13 action space size 80 action 40\n",
      "episode 1 step 22 reward 0.0 action space size 79 action 76\n",
      "episode 1 step 23 reward 0.0 action space size 82 action 44\n",
      "episode 1 step 24 reward 0.0 action space size 82 action 16\n",
      "episode 1 step 25 reward 2.2737367544323206e-13 action space size 84 action 9\n",
      "episode 1 step 26 reward 4.547473508864641e-13 action space size 85 action 41\n",
      "episode 1 step 27 reward 0.0 action space size 86 action 25\n",
      "episode 1 step 28 reward 4.547473508864641e-13 action space size 88 action 73\n",
      "episode 1 step 29 reward 2.2737367544323206e-13 action space size 89 action 25\n",
      "episode 1 step 30 reward 6.821210263296962e-13 action space size 91 action 12\n",
      "episode 1 step 31 reward 2.2737367544323206e-13 action space size 91 action 81\n",
      "episode 1 step 32 reward 4.547473508864641e-13 action space size 89 action 16\n",
      "episode 1 step 33 reward 2.2737367544323206e-13 action space size 93 action 67\n",
      "episode 1 step 34 reward 2.2737367544323206e-13 action space size 94 action 68\n",
      "episode 1 step 35 reward 0.0 action space size 91 action 43\n",
      "episode 1 step 36 reward 2.2737367544323206e-13 action space size 97 action 40\n",
      "episode 1 step 37 reward 4.547473508864641e-13 action space size 98 action 10\n",
      "episode 1 step 38 reward 2.2737367544323206e-13 action space size 98 action 56\n",
      "error in lp iteration\n",
      "episode 1 step 39 reward 0.0 action space size 98 action 2\n",
      "episode 2 step 0 reward 0.0002109746392306988 action space size 59 action 40\n",
      "episode 2 step 1 reward 0.0002282481855218066 action space size 63 action 49\n",
      "episode 2 step 2 reward 0.026291937021596823 action space size 64 action 52\n",
      "episode 2 step 3 reward 0.005431637740457518 action space size 65 action 28\n",
      "episode 2 step 4 reward 0.014261969767630944 action space size 66 action 3\n",
      "episode 2 step 5 reward 0.006325348440441303 action space size 67 action 65\n",
      "episode 2 step 6 reward 8.37412262626458e-05 action space size 68 action 7\n",
      "episode 2 step 7 reward 0.0004218659901198407 action space size 67 action 8\n",
      "episode 2 step 8 reward 0.0020550151893985458 action space size 69 action 57\n",
      "episode 2 step 9 reward 0.006808361853018141 action space size 67 action 23\n",
      "episode 2 step 10 reward 0.019503623896525824 action space size 72 action 44\n",
      "episode 2 step 11 reward 0.003094427024734614 action space size 73 action 42\n",
      "episode 2 step 12 reward 0.006159614746593434 action space size 73 action 37\n",
      "episode 2 step 13 reward 0.0004463207133085234 action space size 74 action 43\n",
      "episode 2 step 14 reward 0.001467379304813221 action space size 76 action 41\n",
      "episode 2 step 15 reward 0.007839379711640504 action space size 75 action 15\n",
      "error in lp iteration\n",
      "episode 2 step 16 reward 0.0 action space size 75 action 32\n",
      "episode 3 step 0 reward 0.00658134072023131 action space size 62 action 37\n",
      "episode 3 step 1 reward 0.02536055356495126 action space size 63 action 4\n",
      "episode 3 step 2 reward 0.002164717001051031 action space size 64 action 5\n",
      "episode 3 step 3 reward 0.032379120670157135 action space size 63 action 63\n",
      "episode 3 step 4 reward 0.00014305174022410938 action space size 66 action 38\n",
      "episode 3 step 5 reward 0.000263312211700395 action space size 67 action 3\n",
      "episode 3 step 6 reward 0.04817184377475314 action space size 64 action 0\n",
      "episode 3 step 7 reward 2.2737367544323206e-13 action space size 67 action 50\n",
      "episode 3 step 8 reward 2.2737367544323206e-13 action space size 67 action 5\n",
      "episode 3 step 9 reward 0.0 action space size 70 action 52\n",
      "episode 3 step 10 reward 2.2737367544323206e-13 action space size 70 action 10\n",
      "episode 3 step 11 reward 2.2737367544323206e-13 action space size 68 action 41\n",
      "episode 3 step 12 reward 0.0 action space size 69 action 48\n",
      "episode 3 step 13 reward 0.0010290524921856559 action space size 73 action 45\n",
      "episode 3 step 14 reward 0.0016326483791999635 action space size 73 action 48\n",
      "episode 3 step 15 reward 0.0008690619354183582 action space size 74 action 42\n",
      "episode 3 step 16 reward 0.018587998698876618 action space size 74 action 68\n",
      "episode 3 step 17 reward 0.0011643833620382793 action space size 77 action 73\n",
      "episode 3 step 18 reward 0.001277267008617855 action space size 78 action 39\n",
      "episode 3 step 19 reward 0.0014145946688586264 action space size 78 action 71\n",
      "episode 3 step 20 reward 0.00015658670668017294 action space size 82 action 20\n",
      "episode 3 step 21 reward 0.002396146902356122 action space size 83 action 21\n",
      "episode 3 step 22 reward 1.3439342637866503e-05 action space size 83 action 65\n",
      "episode 3 step 23 reward 0.00041600852046030923 action space size 85 action 51\n",
      "episode 3 step 24 reward 0.000996050550156724 action space size 82 action 42\n",
      "episode 3 step 25 reward 0.0012711652957477781 action space size 86 action 53\n",
      "episode 3 step 26 reward 0.00026688219281822967 action space size 83 action 66\n",
      "episode 3 step 27 reward 0.0004239271049755189 action space size 89 action 21\n",
      "episode 3 step 28 reward 0.00010083523784487625 action space size 86 action 82\n",
      "episode 3 step 29 reward 0.00044456239402279607 action space size 89 action 53\n",
      "episode 3 step 30 reward 5.22839295626909e-05 action space size 91 action 67\n",
      "episode 3 step 31 reward 0.00022896639234204486 action space size 92 action 27\n",
      "error in lp iteration\n",
      "episode 3 step 32 reward 0.0 action space size 92 action 75\n",
      "episode 4 step 0 reward 0.06125678128319123 action space size 61 action 9\n",
      "episode 4 step 1 reward 0.029095282099660835 action space size 62 action 57\n",
      "episode 4 step 2 reward 0.005399001132900594 action space size 64 action 44\n",
      "episode 4 step 3 reward 0.034855581039664685 action space size 62 action 9\n",
      "episode 4 step 4 reward 0.00036056493081559893 action space size 65 action 4\n",
      "episode 4 step 5 reward 0.0025216991966772184 action space size 65 action 15\n",
      "episode 4 step 6 reward 0.00011328663958920515 action space size 67 action 53\n",
      "episode 4 step 7 reward 0.002310484276222269 action space size 67 action 11\n",
      "episode 4 step 8 reward 0.003675435736113286 action space size 69 action 19\n",
      "episode 4 step 9 reward 0.00025703346318550757 action space size 68 action 13\n",
      "episode 4 step 10 reward 4.041301053803181e-06 action space size 70 action 20\n",
      "episode 4 step 11 reward 0.00028039351400366286 action space size 72 action 24\n",
      "episode 4 step 12 reward 0.0002798043678922113 action space size 70 action 33\n",
      "episode 4 step 13 reward 0.0007331704937314498 action space size 72 action 10\n",
      "episode 4 step 14 reward 0.00036526038820738904 action space size 75 action 4\n",
      "episode 4 step 15 reward 0.00012245271409483394 action space size 76 action 49\n",
      "episode 4 step 16 reward 0.00042311243078074767 action space size 76 action 65\n",
      "episode 4 step 17 reward 0.0005849209724146931 action space size 77 action 27\n",
      "episode 4 step 18 reward 8.772066394158173e-06 action space size 80 action 69\n",
      "episode 4 step 19 reward 2.9110434297763277e-05 action space size 78 action 2\n",
      "episode 4 step 20 reward 1.2890039215562865e-05 action space size 80 action 35\n",
      "episode 4 step 21 reward 4.980907579010818e-05 action space size 82 action 54\n",
      "episode 4 step 22 reward 2.2105564312369097e-06 action space size 84 action 31\n",
      "episode 4 step 23 reward 1.3551061783800833e-07 action space size 83 action 4\n",
      "episode 4 step 24 reward 1.0934863894362934e-06 action space size 86 action 42\n",
      "episode 4 step 25 reward 1.014015788314282e-05 action space size 86 action 24\n",
      "episode 4 step 26 reward 6.686177493975265e-05 action space size 83 action 33\n",
      "episode 4 step 27 reward 1.066470622390625e-05 action space size 89 action 82\n",
      "episode 4 step 28 reward 0.00017360919036946143 action space size 90 action 85\n",
      "episode 4 step 29 reward 1.8382884263701271e-06 action space size 88 action 88\n",
      "episode 4 step 30 reward 1.4330253179650754e-05 action space size 88 action 31\n",
      "episode 4 step 31 reward 0.0002404906213087088 action space size 92 action 19\n",
      "episode 4 step 32 reward 1.9381651327421423e-05 action space size 92 action 71\n",
      "episode 4 step 33 reward 0.0001967181947293284 action space size 94 action 55\n",
      "episode 4 step 34 reward 0.00015285245308405138 action space size 89 action 18\n",
      "episode 4 step 35 reward 0.00011030867835870595 action space size 96 action 35\n",
      "episode 4 step 36 reward 1.0893194485106505e-05 action space size 96 action 77\n",
      "episode 4 step 37 reward 3.1120302992349025e-06 action space size 96 action 63\n",
      "episode 4 step 38 reward 1.9223835806769785e-05 action space size 99 action 46\n",
      "episode 4 step 39 reward 0.0003364016160958272 action space size 100 action 8\n",
      "episode 4 step 40 reward 9.695022527012043e-06 action space size 98 action 19\n",
      "episode 4 step 41 reward 0.0013120862126925203 action space size 101 action 35\n",
      "episode 4 step 42 reward 0.0002003608401537349 action space size 101 action 57\n",
      "episode 4 step 43 reward 0.0002715329978855152 action space size 103 action 83\n",
      "episode 4 step 44 reward 0.00012297944840611308 action space size 104 action 51\n",
      "episode 4 step 45 reward 0.00013809459960612003 action space size 104 action 40\n",
      "episode 4 step 46 reward 7.131248730729567e-05 action space size 104 action 102\n",
      "episode 4 step 47 reward 0.000627289034127898 action space size 106 action 17\n",
      "episode 4 step 48 reward 0.0009659720544732409 action space size 109 action 97\n",
      "episode 4 step 49 reward 3.2680095500836615e-06 action space size 110 action 58\n",
      "episode 5 step 0 reward 0.007805409814864106 action space size 62 action 37\n",
      "episode 5 step 1 reward 0.005722916781905951 action space size 63 action 4\n",
      "episode 5 step 2 reward 0.003001875265908893 action space size 62 action 16\n",
      "episode 5 step 3 reward 0.0008518617028130393 action space size 60 action 10\n",
      "episode 5 step 4 reward 0.016155802885350568 action space size 65 action 33\n",
      "episode 5 step 5 reward 0.0038024683180992724 action space size 61 action 7\n",
      "episode 5 step 6 reward 0.0019745663917092315 action space size 67 action 56\n",
      "episode 5 step 7 reward 0.001731152277898218 action space size 69 action 27\n",
      "episode 5 step 8 reward 0.00028010545838697 action space size 68 action 22\n",
      "episode 5 step 9 reward 0.001057720249718841 action space size 70 action 14\n",
      "episode 5 step 10 reward 0.00035051291752097313 action space size 72 action 66\n",
      "episode 5 step 11 reward 0.00043827183162648 action space size 73 action 22\n",
      "episode 5 step 12 reward 0.006375872910666658 action space size 73 action 59\n",
      "episode 5 step 13 reward 0.006819264485329768 action space size 71 action 31\n",
      "episode 5 step 14 reward 0.00018272291799803497 action space size 74 action 46\n",
      "episode 5 step 15 reward 0.0005619654061774781 action space size 74 action 52\n",
      "episode 5 step 16 reward 0.0007643629505764693 action space size 78 action 19\n",
      "episode 5 step 17 reward 4.197196494715172e-05 action space size 78 action 63\n",
      "episode 5 step 18 reward 1.3885272892366629e-05 action space size 79 action 11\n",
      "episode 5 step 19 reward 1.6852975477377186e-05 action space size 77 action 78\n",
      "episode 5 step 20 reward 9.933417231877684e-05 action space size 81 action 44\n",
      "episode 5 step 21 reward 0.003032625503692543 action space size 81 action 33\n",
      "episode 5 step 22 reward 0.0027725597824428405 action space size 81 action 14\n",
      "episode 5 step 23 reward 0.0010705751096793392 action space size 82 action 7\n",
      "episode 5 step 24 reward 0.0026732964415714378 action space size 85 action 68\n",
      "episode 5 step 25 reward 1.951621743501164e-05 action space size 85 action 67\n",
      "episode 5 step 26 reward 3.7960518056934234e-05 action space size 86 action 64\n",
      "episode 5 step 27 reward 0.001803009330160421 action space size 88 action 1\n",
      "episode 5 step 28 reward 0.002012588626712386 action space size 86 action 56\n",
      "episode 5 step 29 reward 0.002022591748755076 action space size 91 action 14\n",
      "episode 5 step 30 reward 0.0007303008515009424 action space size 90 action 50\n",
      "episode 5 step 31 reward 7.360633389907889e-06 action space size 91 action 62\n",
      "episode 5 step 32 reward 1.7715033209242392e-06 action space size 91 action 7\n",
      "episode 5 step 33 reward 3.872283514283481e-05 action space size 94 action 74\n",
      "episode 5 step 34 reward 0.0005692043459930574 action space size 95 action 39\n",
      "episode 5 step 35 reward 9.714899579194025e-05 action space size 94 action 86\n",
      "episode 5 step 36 reward 2.9369908588705584e-05 action space size 98 action 69\n",
      "episode 5 step 37 reward 0.0008813234244371415 action space size 95 action 52\n",
      "episode 5 step 38 reward 1.8164102129958337e-05 action space size 98 action 54\n",
      "episode 5 step 39 reward 0.00020309465708123753 action space size 100 action 19\n",
      "episode 5 step 40 reward 5.70405518374173e-07 action space size 98 action 58\n",
      "episode 5 step 41 reward 0.0002753862659119477 action space size 101 action 33\n",
      "episode 5 step 42 reward 0.0005365415072446922 action space size 101 action 17\n",
      "episode 5 step 43 reward 5.9507197420316515e-05 action space size 103 action 49\n",
      "episode 5 step 44 reward 0.00043220005863986444 action space size 105 action 61\n",
      "episode 5 step 45 reward 3.596542092054733e-05 action space size 103 action 39\n",
      "episode 5 step 46 reward 0.00013142096076990128 action space size 105 action 72\n",
      "episode 5 step 47 reward 2.7247720026934985e-06 action space size 108 action 11\n",
      "episode 5 step 48 reward 0.00015454840468009934 action space size 109 action 93\n",
      "episode 5 step 49 reward 0.0001512951603217516 action space size 106 action 43\n",
      "episode 6 step 0 reward 0.03254296463705941 action space size 62 action 44\n",
      "episode 6 step 1 reward 0.007929936175287366 action space size 63 action 52\n",
      "episode 6 step 2 reward 0.01743943856513397 action space size 63 action 34\n",
      "episode 6 step 3 reward 0.06799394550102988 action space size 64 action 39\n",
      "episode 6 step 4 reward 0.013650104455109613 action space size 65 action 24\n",
      "episode 6 step 5 reward 0.0003770366099615785 action space size 66 action 15\n",
      "episode 6 step 6 reward 0.016003651846631328 action space size 66 action 56\n",
      "episode 6 step 7 reward 0.0002915261634370836 action space size 66 action 29\n",
      "episode 6 step 8 reward 0.010959531730804883 action space size 68 action 31\n",
      "episode 6 step 9 reward 0.0015324923319894879 action space size 69 action 16\n",
      "episode 6 step 10 reward 0.0009704888500436937 action space size 72 action 3\n",
      "episode 6 step 11 reward 3.014237699972e-05 action space size 70 action 8\n",
      "episode 6 step 12 reward 0.00016114610002659902 action space size 72 action 3\n",
      "episode 6 step 13 reward 1.1869831496369443e-05 action space size 71 action 51\n",
      "episode 6 step 14 reward 0.00016908538145798957 action space size 75 action 24\n",
      "episode 6 step 15 reward 0.0017627729203013587 action space size 77 action 7\n",
      "episode 6 step 16 reward 0.0009351792396046221 action space size 76 action 18\n",
      "episode 6 step 17 reward 0.000563004399282363 action space size 77 action 6\n",
      "episode 6 step 18 reward 8.954607210398535e-05 action space size 80 action 26\n",
      "episode 6 step 19 reward 2.2681207838104456e-05 action space size 81 action 67\n",
      "episode 6 step 20 reward 0.002754539957322777 action space size 80 action 9\n",
      "episode 6 step 21 reward 4.089728918188484e-06 action space size 83 action 29\n",
      "episode 6 step 22 reward 0.0004911038959107827 action space size 83 action 7\n",
      "episode 6 step 23 reward 2.3560104409625637e-05 action space size 84 action 78\n",
      "episode 6 step 24 reward 0.0001177564254248864 action space size 84 action 28\n",
      "episode 6 step 25 reward 1.8799681129166856e-05 action space size 87 action 54\n",
      "episode 6 step 26 reward 3.3368120739396545e-05 action space size 86 action 63\n",
      "episode 6 step 27 reward 7.706104952376336e-06 action space size 88 action 81\n",
      "episode 6 step 28 reward 1.8174338265453116e-05 action space size 90 action 84\n",
      "episode 6 step 29 reward 0.00018267214522893482 action space size 88 action 51\n",
      "episode 6 step 30 reward 0.00021100455046507705 action space size 91 action 45\n",
      "episode 6 step 31 reward 9.992578111450712e-05 action space size 89 action 15\n",
      "episode 6 step 32 reward 3.442949173404486e-05 action space size 87 action 53\n",
      "episode 6 step 33 reward 0.00012657128945647855 action space size 94 action 40\n",
      "episode 6 step 34 reward 6.605107728319126e-06 action space size 95 action 39\n",
      "episode 6 step 35 reward 0.0005864234210548602 action space size 93 action 62\n",
      "episode 6 step 36 reward 1.484643735238933e-05 action space size 96 action 75\n",
      "episode 6 step 37 reward 8.612823876319453e-08 action space size 96 action 48\n",
      "episode 6 step 38 reward 1.0271548944729147e-06 action space size 99 action 2\n",
      "episode 6 step 39 reward 6.827198376413435e-05 action space size 98 action 75\n",
      "episode 6 step 40 reward 0.00013109371730024577 action space size 101 action 46\n",
      "episode 6 step 41 reward 1.2522746374088456e-05 action space size 102 action 28\n",
      "episode 6 step 42 reward 1.647621274969424e-05 action space size 98 action 59\n",
      "episode 6 step 43 reward 3.983215037806076e-06 action space size 104 action 27\n",
      "episode 6 step 44 reward 2.198059814872977e-05 action space size 105 action 67\n",
      "episode 6 step 45 reward 0.00015837298110454867 action space size 107 action 18\n",
      "episode 6 step 46 reward 5.774954615844763e-06 action space size 107 action 89\n",
      "episode 6 step 47 reward 8.690449249115773e-09 action space size 106 action 50\n",
      "episode 6 step 48 reward 1.3083781823297613e-05 action space size 110 action 34\n",
      "episode 6 step 49 reward 4.15325928315724e-05 action space size 109 action 52\n",
      "episode 7 step 0 reward 0.11534908185194581 action space size 61 action 0\n",
      "episode 7 step 1 reward 4.547473508864641e-13 action space size 59 action 0\n",
      "episode 7 step 2 reward 0.0 action space size 62 action 21\n",
      "episode 7 step 3 reward 0.0 action space size 63 action 20\n",
      "episode 7 step 4 reward 9.094947017729282e-13 action space size 65 action 40\n",
      "episode 7 step 5 reward 9.094947017729282e-13 action space size 66 action 16\n",
      "episode 7 step 6 reward 9.094947017729282e-13 action space size 63 action 24\n",
      "episode 7 step 7 reward 1.3642420526593924e-12 action space size 67 action 37\n",
      "episode 7 step 8 reward 4.547473508864641e-13 action space size 68 action 30\n",
      "episode 7 step 9 reward 0.0 action space size 65 action 64\n",
      "episode 7 step 10 reward 0.0 action space size 70 action 34\n",
      "episode 7 step 11 reward 4.547473508864641e-13 action space size 71 action 13\n",
      "episode 7 step 12 reward 4.547473508864641e-13 action space size 71 action 57\n",
      "episode 7 step 13 reward 9.094947017729282e-13 action space size 74 action 40\n",
      "episode 7 step 14 reward 4.547473508864641e-13 action space size 74 action 17\n",
      "episode 7 step 15 reward 4.547473508864641e-13 action space size 75 action 42\n",
      "episode 7 step 16 reward 9.094947017729282e-13 action space size 77 action 7\n",
      "episode 7 step 17 reward 9.094947017729282e-13 action space size 78 action 0\n",
      "episode 7 step 18 reward 4.547473508864641e-13 action space size 77 action 35\n",
      "episode 7 step 19 reward 4.547473508864641e-13 action space size 78 action 11\n",
      "episode 7 step 20 reward 0.0 action space size 79 action 44\n",
      "episode 7 step 21 reward 4.547473508864641e-13 action space size 82 action 48\n",
      "episode 7 step 22 reward 9.094947017729282e-13 action space size 82 action 69\n",
      "episode 7 step 23 reward 4.547473508864641e-13 action space size 83 action 76\n",
      "episode 7 step 24 reward 0.0 action space size 81 action 74\n",
      "episode 7 step 25 reward 0.0 action space size 83 action 32\n",
      "episode 7 step 26 reward 0.0 action space size 85 action 37\n",
      "episode 7 step 27 reward 0.0 action space size 83 action 55\n",
      "episode 7 step 28 reward 0.0 action space size 86 action 42\n",
      "episode 7 step 29 reward 4.547473508864641e-13 action space size 89 action 35\n",
      "episode 7 step 30 reward 4.547473508864641e-13 action space size 91 action 13\n",
      "episode 7 step 31 reward 1.3642420526593924e-12 action space size 89 action 1\n",
      "episode 7 step 32 reward 9.094947017729282e-13 action space size 90 action 80\n",
      "episode 7 step 33 reward 4.547473508864641e-13 action space size 91 action 15\n",
      "episode 7 step 34 reward 4.547473508864641e-13 action space size 92 action 62\n",
      "episode 7 step 35 reward 4.547473508864641e-13 action space size 93 action 79\n",
      "episode 7 step 36 reward 4.547473508864641e-13 action space size 95 action 80\n",
      "episode 7 step 37 reward 9.094947017729282e-13 action space size 96 action 2\n",
      "episode 7 step 38 reward 4.547473508864641e-13 action space size 99 action 88\n",
      "episode 7 step 39 reward 0.0 action space size 100 action 20\n",
      "episode 7 step 40 reward 0.0 action space size 101 action 63\n",
      "episode 7 step 41 reward 0.0 action space size 98 action 81\n",
      "episode 7 step 42 reward 4.547473508864641e-13 action space size 99 action 90\n",
      "episode 7 step 43 reward 9.094947017729282e-13 action space size 99 action 38\n",
      "episode 7 step 44 reward 9.094947017729282e-13 action space size 104 action 26\n",
      "episode 7 step 45 reward 4.547473508864641e-13 action space size 105 action 48\n",
      "episode 7 step 46 reward 4.547473508864641e-13 action space size 104 action 61\n",
      "episode 7 step 47 reward 9.094947017729282e-13 action space size 105 action 96\n",
      "episode 7 step 48 reward 4.547473508864641e-13 action space size 106 action 53\n",
      "episode 7 step 49 reward 4.547473508864641e-13 action space size 108 action 64\n",
      "episode 8 step 0 reward 0.0008093529313555337 action space size 60 action 29\n",
      "episode 8 step 1 reward 0.006553749249178509 action space size 58 action 40\n",
      "episode 8 step 2 reward 0.0025452168233641714 action space size 61 action 43\n",
      "episode 8 step 3 reward 0.0023145472855503613 action space size 63 action 2\n",
      "episode 8 step 4 reward 0.001252858700354409 action space size 66 action 40\n",
      "episode 8 step 5 reward 0.0009638770934543572 action space size 64 action 56\n",
      "episode 8 step 6 reward 0.00025652227668615524 action space size 68 action 33\n",
      "episode 8 step 7 reward 0.6911500265618997 action space size 66 action 0\n",
      "episode 8 step 8 reward 4.547473508864641e-13 action space size 64 action 9\n",
      "episode 8 step 9 reward 4.547473508864641e-13 action space size 69 action 33\n",
      "episode 8 step 10 reward 4.547473508864641e-13 action space size 68 action 24\n",
      "episode 8 step 11 reward 0.0 action space size 68 action 25\n",
      "episode 8 step 12 reward 0.0 action space size 72 action 31\n",
      "episode 8 step 13 reward 0.0 action space size 74 action 62\n",
      "episode 8 step 14 reward 0.0 action space size 73 action 27\n",
      "episode 8 step 15 reward 0.0 action space size 76 action 58\n",
      "episode 8 step 16 reward 0.0 action space size 76 action 72\n",
      "episode 8 step 17 reward 4.547473508864641e-13 action space size 77 action 62\n",
      "episode 8 step 18 reward 4.547473508864641e-13 action space size 78 action 9\n",
      "episode 8 step 19 reward 0.0 action space size 78 action 23\n",
      "episode 8 step 20 reward 0.0 action space size 79 action 33\n",
      "episode 8 step 21 reward 4.547473508864641e-13 action space size 81 action 47\n",
      "episode 8 step 22 reward 4.547473508864641e-13 action space size 81 action 42\n",
      "episode 8 step 23 reward 0.0 action space size 82 action 79\n",
      "episode 8 step 24 reward 0.0 action space size 83 action 46\n",
      "episode 8 step 25 reward 0.0 action space size 85 action 28\n",
      "episode 8 step 26 reward 0.0 action space size 86 action 42\n",
      "episode 8 step 27 reward 4.547473508864641e-13 action space size 86 action 69\n",
      "episode 8 step 28 reward 4.547473508864641e-13 action space size 87 action 29\n",
      "episode 8 step 29 reward 0.0 action space size 90 action 40\n",
      "episode 8 step 30 reward 4.547473508864641e-13 action space size 88 action 16\n",
      "episode 8 step 31 reward 0.0 action space size 90 action 84\n",
      "episode 8 step 32 reward 0.0 action space size 91 action 82\n",
      "episode 8 step 33 reward 4.547473508864641e-13 action space size 92 action 1\n",
      "episode 8 step 34 reward 4.547473508864641e-13 action space size 93 action 72\n",
      "episode 8 step 35 reward 4.547473508864641e-13 action space size 95 action 41\n",
      "episode 8 step 36 reward 0.0 action space size 95 action 64\n",
      "episode 8 step 37 reward 4.547473508864641e-13 action space size 97 action 78\n",
      "episode 8 step 38 reward 4.547473508864641e-13 action space size 97 action 31\n",
      "episode 8 step 39 reward 4.547473508864641e-13 action space size 97 action 81\n",
      "episode 8 step 40 reward 4.547473508864641e-13 action space size 96 action 19\n",
      "episode 8 step 41 reward 4.547473508864641e-13 action space size 99 action 65\n",
      "episode 8 step 42 reward 9.094947017729282e-13 action space size 102 action 45\n",
      "episode 8 step 43 reward 4.547473508864641e-13 action space size 102 action 92\n",
      "episode 8 step 44 reward 0.0 action space size 101 action 23\n",
      "episode 8 step 45 reward 4.547473508864641e-13 action space size 101 action 76\n",
      "episode 8 step 46 reward 4.547473508864641e-13 action space size 103 action 16\n",
      "episode 8 step 47 reward 0.0 action space size 106 action 49\n",
      "episode 8 step 48 reward 0.0 action space size 108 action 58\n",
      "episode 8 step 49 reward 0.0 action space size 106 action 38\n",
      "episode 9 step 0 reward 0.015931637564335688 action space size 60 action 23\n",
      "episode 9 step 1 reward 0.023555371538805048 action space size 62 action 23\n",
      "episode 9 step 2 reward 0.04049470638346975 action space size 63 action 26\n",
      "episode 9 step 3 reward 0.004445048176421551 action space size 64 action 20\n",
      "episode 9 step 4 reward 0.00010753215428849217 action space size 65 action 36\n",
      "episode 9 step 5 reward 0.012868547938523989 action space size 65 action 29\n",
      "episode 9 step 6 reward 0.002843224177013326 action space size 67 action 34\n",
      "episode 9 step 7 reward 0.0015810338791197864 action space size 66 action 59\n",
      "episode 9 step 8 reward 0.001363109082376468 action space size 70 action 22\n",
      "episode 9 step 9 reward 0.005631361183532135 action space size 69 action 52\n",
      "episode 9 step 10 reward 0.004428732231644972 action space size 71 action 15\n",
      "episode 9 step 11 reward 0.0016508232697560743 action space size 71 action 11\n",
      "episode 9 step 12 reward 0.00033200862071680604 action space size 72 action 30\n",
      "episode 9 step 13 reward 0.00034723982844298007 action space size 74 action 64\n",
      "episode 9 step 14 reward 0.003750067420241976 action space size 75 action 42\n",
      "episode 9 step 15 reward 2.985496485052863e-05 action space size 75 action 59\n",
      "episode 9 step 16 reward 0.00046129341990308603 action space size 75 action 68\n",
      "episode 9 step 17 reward 2.4963590476545505e-05 action space size 77 action 19\n",
      "episode 9 step 18 reward 0.006001626908528124 action space size 80 action 37\n",
      "episode 9 step 19 reward 0.00163301792645143 action space size 78 action 70\n",
      "episode 9 step 20 reward 0.0004182542625130736 action space size 82 action 43\n",
      "episode 9 step 21 reward 0.0002520955076761311 action space size 82 action 74\n",
      "episode 9 step 22 reward 0.00014712433357999544 action space size 80 action 33\n",
      "episode 9 step 23 reward 5.149584785613115e-05 action space size 84 action 61\n",
      "episode 9 step 24 reward 2.00813583433046e-05 action space size 85 action 47\n",
      "episode 9 step 25 reward 0.00010526030473556602 action space size 85 action 84\n",
      "episode 9 step 26 reward 0.00023341935320786433 action space size 86 action 69\n",
      "episode 9 step 27 reward 0.0005181679007364437 action space size 88 action 2\n",
      "episode 9 step 28 reward 0.000110880177544459 action space size 88 action 81\n",
      "episode 9 step 29 reward 4.606775655702222e-05 action space size 90 action 7\n",
      "episode 9 step 30 reward 0.00018354469875703217 action space size 90 action 78\n",
      "episode 9 step 31 reward 0.0002907554298872128 action space size 93 action 37\n",
      "episode 9 step 32 reward 0.000526001137586718 action space size 94 action 36\n",
      "episode 9 step 33 reward 0.0006798410258852527 action space size 92 action 65\n",
      "episode 9 step 34 reward 0.00016109838497868623 action space size 95 action 20\n",
      "episode 9 step 35 reward 0.00011532340886333259 action space size 93 action 68\n",
      "episode 9 step 36 reward 0.0006391433748831332 action space size 98 action 87\n",
      "episode 9 step 37 reward 7.238818352561793e-05 action space size 92 action 11\n",
      "episode 9 step 38 reward 9.514604334981414e-05 action space size 99 action 25\n",
      "episode 9 step 39 reward 5.421070090960711e-05 action space size 99 action 53\n",
      "episode 9 step 40 reward 0.0002456446986798255 action space size 101 action 15\n",
      "episode 9 step 41 reward 6.453397554651019e-05 action space size 100 action 18\n",
      "episode 9 step 42 reward 0.0002416643496871984 action space size 104 action 27\n",
      "episode 9 step 43 reward 5.469113693834515e-05 action space size 105 action 52\n",
      "episode 9 step 44 reward 8.473702018818585e-05 action space size 104 action 4\n",
      "episode 9 step 45 reward 0.00010367869481342495 action space size 104 action 99\n",
      "episode 9 step 46 reward 0.0002033934142673388 action space size 108 action 38\n",
      "episode 9 step 47 reward 3.1549081541015767e-05 action space size 107 action 42\n",
      "episode 9 step 48 reward 5.578686341323191e-06 action space size 107 action 80\n",
      "episode 9 step 49 reward 8.788761988398619e-06 action space size 108 action 41\n",
      "episode 10 step 0 reward 0.011296038098407735 action space size 59 action 1\n",
      "episode 10 step 1 reward 0.002228286610716168 action space size 60 action 5\n",
      "episode 10 step 2 reward 0.002699381695492775 action space size 60 action 43\n",
      "episode 10 step 3 reward 0.011829368102553417 action space size 64 action 12\n",
      "episode 10 step 4 reward 0.01605401849565169 action space size 65 action 2\n",
      "episode 10 step 5 reward 0.006986825066633173 action space size 67 action 10\n",
      "episode 10 step 6 reward 0.005256363183434587 action space size 62 action 38\n",
      "episode 10 step 7 reward 8.767934195930138e-05 action space size 68 action 36\n",
      "episode 10 step 8 reward 0.0010527799649935332 action space size 69 action 21\n",
      "episode 10 step 9 reward 0.0008163369498106476 action space size 70 action 15\n",
      "episode 10 step 10 reward 0.008773344295605057 action space size 70 action 47\n",
      "episode 10 step 11 reward 0.0017454710582569533 action space size 72 action 15\n",
      "episode 10 step 12 reward 0.0009590542977093719 action space size 74 action 52\n",
      "episode 10 step 13 reward 0.005689285906100849 action space size 75 action 62\n",
      "episode 10 step 14 reward 0.0005108497998662642 action space size 75 action 20\n",
      "episode 10 step 15 reward 0.0004036739273942658 action space size 77 action 24\n",
      "episode 10 step 16 reward 5.86863779972191e-06 action space size 77 action 59\n",
      "episode 10 step 17 reward 0.0001755901121214265 action space size 77 action 75\n",
      "episode 10 step 18 reward 3.17632193400641e-05 action space size 78 action 68\n",
      "episode 10 step 19 reward 9.866129266811186e-05 action space size 75 action 63\n",
      "episode 10 step 20 reward 0.0002866272129722347 action space size 80 action 33\n",
      "episode 10 step 21 reward 0.00020734051713588997 action space size 81 action 78\n",
      "episode 10 step 22 reward 0.0001343679964520561 action space size 84 action 37\n",
      "episode 10 step 23 reward 0.0001955717129931145 action space size 82 action 26\n",
      "episode 10 step 24 reward 6.397326615115162e-05 action space size 84 action 30\n",
      "episode 10 step 25 reward 2.671181164259906e-05 action space size 86 action 5\n",
      "episode 10 step 26 reward 6.593043235625373e-05 action space size 86 action 45\n",
      "episode 10 step 27 reward 0.0005685338423973008 action space size 86 action 67\n",
      "episode 10 step 28 reward 7.127359367586905e-05 action space size 87 action 81\n",
      "episode 10 step 29 reward 0.00041018876754606026 action space size 89 action 77\n",
      "episode 10 step 30 reward 1.1586562777665677e-05 action space size 92 action 62\n",
      "episode 10 step 31 reward 1.7912289877131116e-05 action space size 89 action 11\n",
      "episode 10 step 32 reward 2.1189689505263232e-05 action space size 94 action 17\n",
      "episode 10 step 33 reward 9.613011161491158e-05 action space size 92 action 90\n",
      "episode 10 step 34 reward 0.0003460086054474232 action space size 94 action 67\n",
      "episode 10 step 35 reward 0.00010295797756043612 action space size 95 action 36\n",
      "episode 10 step 36 reward 0.0005126289061081479 action space size 97 action 47\n",
      "episode 10 step 37 reward 0.00013776901141682174 action space size 97 action 10\n",
      "episode 10 step 38 reward 4.089771664439468e-05 action space size 98 action 73\n",
      "episode 10 step 39 reward 0.00016672774518156075 action space size 98 action 58\n",
      "episode 10 step 40 reward 1.350413185718935e-05 action space size 99 action 6\n",
      "episode 10 step 41 reward 0.00021899827152083162 action space size 99 action 14\n",
      "episode 10 step 42 reward 0.00014986972246333607 action space size 103 action 84\n",
      "episode 10 step 43 reward 6.039733852958307e-05 action space size 103 action 92\n",
      "episode 10 step 44 reward 9.062269100468257e-05 action space size 103 action 80\n",
      "episode 10 step 45 reward 0.0007665180974072427 action space size 105 action 1\n",
      "episode 10 step 46 reward 4.55922304354317e-05 action space size 104 action 14\n",
      "episode 10 step 47 reward 1.5256791357387556e-05 action space size 106 action 90\n",
      "episode 10 step 48 reward 2.9459111374308122e-05 action space size 107 action 49\n",
      "episode 10 step 49 reward 2.8243771339475643e-06 action space size 108 action 79\n",
      "episode 11 step 0 reward 0.02052139816419185 action space size 61 action 38\n",
      "episode 11 step 1 reward 0.0029042223554824886 action space size 62 action 28\n",
      "episode 11 step 2 reward 0.022729080325916584 action space size 58 action 12\n",
      "episode 11 step 3 reward 0.0016643522480990214 action space size 63 action 2\n",
      "episode 11 step 4 reward 0.0003020169565388642 action space size 65 action 27\n",
      "episode 11 step 5 reward 0.008659020546929241 action space size 65 action 62\n",
      "episode 11 step 6 reward 0.00016442880655631598 action space size 68 action 23\n",
      "episode 11 step 7 reward 0.001980354250235905 action space size 68 action 9\n",
      "episode 11 step 8 reward 0.0002276950094710628 action space size 69 action 37\n",
      "episode 11 step 9 reward 0.0003466380130703328 action space size 69 action 26\n",
      "episode 11 step 10 reward 0.0003355581791311124 action space size 70 action 32\n",
      "episode 11 step 11 reward 1.2512662351582549e-05 action space size 67 action 8\n",
      "episode 11 step 12 reward 0.00015960697919581435 action space size 73 action 62\n",
      "episode 11 step 13 reward 0.00017786217995308107 action space size 74 action 15\n",
      "episode 11 step 14 reward 0.0026437870967583876 action space size 75 action 29\n",
      "episode 11 step 15 reward 4.3957629713986535e-05 action space size 76 action 4\n",
      "episode 11 step 16 reward 0.0019128650401398772 action space size 77 action 58\n",
      "episode 11 step 17 reward 0.0003033552070519363 action space size 76 action 44\n",
      "episode 11 step 18 reward 0.0026525387909259734 action space size 80 action 32\n",
      "episode 11 step 19 reward 0.00017603174228497664 action space size 77 action 64\n",
      "episode 11 step 20 reward 0.00241000258256463 action space size 81 action 34\n",
      "episode 11 step 21 reward 3.3874880500661675e-06 action space size 81 action 27\n",
      "episode 11 step 22 reward 0.851285341313087 action space size 80 action 0\n",
      "episode 11 step 23 reward 2.2737367544323206e-13 action space size 80 action 55\n",
      "episode 11 step 24 reward 2.2737367544323206e-13 action space size 83 action 0\n",
      "episode 11 step 25 reward 0.0 action space size 86 action 10\n",
      "episode 11 step 26 reward 2.2737367544323206e-13 action space size 84 action 74\n",
      "episode 11 step 27 reward 0.0 action space size 87 action 20\n",
      "episode 11 step 28 reward 2.2737367544323206e-13 action space size 88 action 61\n",
      "episode 11 step 29 reward 0.0 action space size 89 action 26\n",
      "episode 11 step 30 reward 2.2737367544323206e-13 action space size 91 action 26\n",
      "episode 11 step 31 reward 0.0 action space size 89 action 79\n",
      "episode 11 step 32 reward 2.2737367544323206e-13 action space size 90 action 67\n",
      "episode 11 step 33 reward 6.821210263296962e-13 action space size 93 action 38\n",
      "episode 11 step 34 reward 2.2737367544323206e-13 action space size 94 action 59\n",
      "episode 11 step 35 reward 2.2737367544323206e-13 action space size 93 action 13\n",
      "episode 11 step 36 reward 2.2737367544323206e-13 action space size 93 action 36\n",
      "episode 11 step 37 reward 2.2737367544323206e-13 action space size 96 action 61\n",
      "episode 11 step 38 reward 0.0 action space size 97 action 67\n",
      "episode 11 step 39 reward 0.0 action space size 99 action 52\n",
      "episode 11 step 40 reward 0.0 action space size 100 action 77\n",
      "episode 11 step 41 reward 2.2737367544323206e-13 action space size 101 action 15\n",
      "episode 11 step 42 reward 4.547473508864641e-13 action space size 100 action 84\n",
      "episode 11 step 43 reward 4.547473508864641e-13 action space size 103 action 33\n",
      "episode 11 step 44 reward 0.0 action space size 105 action 0\n",
      "episode 11 step 45 reward 0.0 action space size 104 action 61\n",
      "episode 11 step 46 reward 0.0 action space size 105 action 101\n",
      "episode 11 step 47 reward 2.2737367544323206e-13 action space size 105 action 83\n",
      "episode 11 step 48 reward 0.0 action space size 107 action 29\n",
      "episode 11 step 49 reward 2.2737367544323206e-13 action space size 105 action 46\n",
      "episode 12 step 0 reward 0.0001455381298001157 action space size 61 action 5\n",
      "episode 12 step 1 reward 0.005372929103941715 action space size 61 action 35\n",
      "episode 12 step 2 reward 6.195520973051316e-05 action space size 63 action 17\n",
      "episode 12 step 3 reward 5.146441253600642e-05 action space size 65 action 41\n",
      "episode 12 step 4 reward 0.0023519470014434773 action space size 66 action 43\n",
      "episode 12 step 5 reward 0.00021399218485385063 action space size 67 action 24\n",
      "episode 12 step 6 reward 0.003129847078525927 action space size 67 action 57\n",
      "episode 12 step 7 reward 0.0013428851375465456 action space size 68 action 13\n",
      "episode 12 step 8 reward 0.0013964019358354562 action space size 70 action 2\n",
      "episode 12 step 9 reward 0.00034438639113432146 action space size 70 action 17\n",
      "episode 12 step 10 reward 0.00029953537068649894 action space size 70 action 61\n",
      "episode 12 step 11 reward 0.008804647779015795 action space size 72 action 31\n",
      "episode 12 step 12 reward 0.00016859761217347113 action space size 73 action 8\n",
      "episode 12 step 13 reward 0.00011301038284727838 action space size 73 action 13\n",
      "episode 12 step 14 reward 0.00019470773531793384 action space size 73 action 45\n",
      "episode 12 step 15 reward 0.00020572221274051117 action space size 75 action 61\n",
      "episode 12 step 16 reward 0.00034493055227358127 action space size 75 action 60\n",
      "episode 12 step 17 reward 0.00016027882975322427 action space size 77 action 67\n",
      "episode 12 step 18 reward 0.00021710436431021662 action space size 80 action 36\n",
      "episode 12 step 19 reward 5.414730412667268e-05 action space size 79 action 13\n",
      "episode 12 step 20 reward 0.001854263451150473 action space size 78 action 41\n",
      "episode 12 step 21 reward 1.4827216546109412e-05 action space size 80 action 40\n",
      "episode 12 step 22 reward 0.0002569365337876661 action space size 81 action 49\n",
      "episode 12 step 23 reward 1.4835656656941865e-05 action space size 84 action 48\n",
      "episode 12 step 24 reward 0.0006822142054261349 action space size 85 action 39\n",
      "episode 12 step 25 reward 0.0007549075307906605 action space size 87 action 16\n",
      "episode 12 step 26 reward 4.633953722077422e-05 action space size 88 action 31\n",
      "episode 12 step 27 reward 6.04981596552534e-05 action space size 88 action 37\n",
      "episode 12 step 28 reward 0.0002476883073541103 action space size 87 action 1\n",
      "episode 12 step 29 reward 3.431544519116869e-05 action space size 90 action 20\n",
      "episode 12 step 30 reward 1.712432413114584e-05 action space size 92 action 51\n",
      "episode 12 step 31 reward 0.00043214410197833786 action space size 92 action 32\n",
      "episode 12 step 32 reward 0.00012249808469277923 action space size 93 action 19\n",
      "episode 12 step 33 reward 0.00041735768536455 action space size 91 action 44\n",
      "episode 12 step 34 reward 0.00017586038984518382 action space size 94 action 61\n",
      "episode 12 step 35 reward 5.630672467304976e-05 action space size 96 action 48\n",
      "episode 12 step 36 reward 1.3938871234131511e-05 action space size 98 action 25\n",
      "episode 12 step 37 reward 1.0855108030227711e-05 action space size 96 action 67\n",
      "episode 12 step 38 reward 0.0008719405541341985 action space size 98 action 2\n",
      "episode 12 step 39 reward 4.19577459069842e-05 action space size 100 action 46\n",
      "episode 12 step 40 reward 1.3835642675985582e-06 action space size 99 action 61\n",
      "episode 12 step 41 reward 5.406159743870376e-06 action space size 101 action 37\n",
      "episode 12 step 42 reward 5.352348398446338e-05 action space size 102 action 73\n",
      "episode 12 step 43 reward 4.782085852639284e-06 action space size 101 action 97\n",
      "episode 12 step 44 reward 0.00023993793820409337 action space size 106 action 12\n",
      "episode 12 step 45 reward 0.0002783980453386903 action space size 106 action 70\n",
      "episode 12 step 46 reward 0.00013014654541620985 action space size 107 action 7\n",
      "episode 12 step 47 reward 5.126678934175288e-05 action space size 108 action 65\n",
      "episode 12 step 48 reward 0.0005151402783667436 action space size 105 action 104\n",
      "episode 12 step 49 reward 0.0003548100858097314 action space size 108 action 20\n",
      "episode 13 step 0 reward 0.004696446445905167 action space size 60 action 18\n",
      "episode 13 step 1 reward 0.04306166961487179 action space size 56 action 54\n",
      "episode 13 step 2 reward 0.026806540393863543 action space size 62 action 44\n",
      "episode 13 step 3 reward 0.0024378098091801803 action space size 65 action 24\n",
      "episode 13 step 4 reward 0.02276558472067336 action space size 64 action 63\n",
      "episode 13 step 5 reward 0.00041093894196819747 action space size 65 action 32\n",
      "episode 13 step 6 reward 0.004296996133234643 action space size 66 action 11\n",
      "episode 13 step 7 reward 0.0004890388868261653 action space size 65 action 13\n",
      "episode 13 step 8 reward 0.007950216990593617 action space size 68 action 37\n",
      "episode 13 step 9 reward 0.008853061059653555 action space size 69 action 42\n",
      "episode 13 step 10 reward 3.0370136755664134e-05 action space size 71 action 14\n",
      "episode 13 step 11 reward 0.0036898943440064613 action space size 72 action 64\n",
      "episode 13 step 12 reward 0.0024919782636061427 action space size 73 action 27\n",
      "episode 13 step 13 reward 0.006220967283752543 action space size 72 action 68\n",
      "episode 13 step 14 reward 9.567410825184197e-05 action space size 75 action 46\n",
      "episode 13 step 15 reward 5.872737801837502e-06 action space size 76 action 59\n",
      "episode 13 step 16 reward 8.829809894450591e-05 action space size 77 action 67\n",
      "episode 13 step 17 reward 0.0004454566437743779 action space size 79 action 19\n",
      "episode 13 step 18 reward 7.49426790207508e-05 action space size 79 action 5\n",
      "episode 13 step 19 reward 0.0005986038804621785 action space size 79 action 53\n",
      "episode 13 step 20 reward 0.000433098092344153 action space size 79 action 45\n",
      "episode 13 step 21 reward 7.049595706121181e-05 action space size 83 action 65\n",
      "episode 13 step 22 reward 2.390998361079255e-05 action space size 83 action 42\n",
      "episode 13 step 23 reward 4.4815205910708755e-06 action space size 85 action 45\n",
      "episode 13 step 24 reward 0.0004638731907107285 action space size 86 action 73\n",
      "episode 13 step 25 reward 6.062239890525234e-05 action space size 85 action 14\n",
      "episode 13 step 26 reward 0.00029554459570135805 action space size 86 action 32\n",
      "episode 13 step 27 reward 2.944903553725453e-05 action space size 89 action 5\n",
      "episode 13 step 28 reward 0.0003147330971842166 action space size 88 action 59\n",
      "episode 13 step 29 reward 0.0009377099981975334 action space size 89 action 16\n",
      "episode 13 step 30 reward 9.81884390967025e-05 action space size 89 action 1\n",
      "episode 13 step 31 reward 0.0004813308532902738 action space size 91 action 9\n",
      "episode 13 step 32 reward 0.0006124110404925887 action space size 92 action 28\n",
      "episode 13 step 33 reward 0.0001218678630721115 action space size 92 action 30\n",
      "episode 13 step 34 reward 1.7471035789640155e-05 action space size 93 action 74\n",
      "episode 13 step 35 reward 4.397931797939236e-05 action space size 93 action 55\n",
      "episode 13 step 36 reward 5.410687617768417e-05 action space size 93 action 89\n",
      "episode 13 step 37 reward 4.340766645327676e-05 action space size 96 action 48\n",
      "episode 13 step 38 reward 0.0004983146850463527 action space size 98 action 33\n",
      "episode 13 step 39 reward 9.500432224740507e-05 action space size 98 action 52\n",
      "error in lp iteration\n",
      "episode 13 step 40 reward 0.0 action space size 98 action 58\n",
      "episode 14 step 0 reward 0.00040126093472281354 action space size 60 action 58\n",
      "episode 14 step 1 reward 0.005888421748522887 action space size 63 action 54\n",
      "episode 14 step 2 reward 0.00029494920931938395 action space size 64 action 40\n",
      "episode 14 step 3 reward 0.0020218177924107295 action space size 63 action 31\n",
      "episode 14 step 4 reward 3.382007275831711e-05 action space size 63 action 10\n",
      "episode 14 step 5 reward 0.0031733983141748467 action space size 67 action 60\n",
      "episode 14 step 6 reward 0.015108123157915543 action space size 67 action 36\n",
      "episode 14 step 7 reward 0.0035469437343635946 action space size 67 action 2\n",
      "episode 14 step 8 reward 0.0008775430467267142 action space size 67 action 56\n",
      "episode 14 step 9 reward 0.004827777250056897 action space size 71 action 30\n",
      "episode 14 step 10 reward 0.0011757684676467761 action space size 71 action 64\n",
      "episode 14 step 11 reward 0.000770502500472503 action space size 71 action 6\n",
      "episode 14 step 12 reward 0.00022249473317970114 action space size 74 action 39\n",
      "episode 14 step 13 reward 0.005566455674852477 action space size 73 action 26\n",
      "episode 14 step 14 reward 0.00011612587809395336 action space size 73 action 36\n",
      "episode 14 step 15 reward 0.0014761337467916746 action space size 74 action 69\n",
      "episode 14 step 16 reward 0.005431814549410774 action space size 78 action 46\n",
      "episode 14 step 17 reward 0.00012954296107636765 action space size 78 action 20\n",
      "episode 14 step 18 reward 0.0001184524508062168 action space size 75 action 73\n",
      "episode 14 step 19 reward 9.67105961535708e-07 action space size 81 action 35\n",
      "episode 14 step 20 reward 0.0016130769074607088 action space size 80 action 46\n",
      "episode 14 step 21 reward 3.3275473469984718e-06 action space size 81 action 13\n",
      "episode 14 step 22 reward 3.019261839654064e-06 action space size 82 action 21\n",
      "episode 14 step 23 reward 0.5623734298746967 action space size 82 action 0\n",
      "episode 14 step 24 reward 0.0 action space size 83 action 50\n",
      "episode 14 step 25 reward 4.547473508864641e-13 action space size 86 action 15\n",
      "episode 14 step 26 reward 0.0 action space size 84 action 26\n",
      "episode 14 step 27 reward 2.2737367544323206e-13 action space size 85 action 10\n",
      "episode 14 step 28 reward 4.547473508864641e-13 action space size 89 action 56\n",
      "episode 14 step 29 reward 0.0 action space size 88 action 88\n",
      "episode 14 step 30 reward 4.547473508864641e-13 action space size 88 action 39\n",
      "episode 14 step 31 reward 4.547473508864641e-13 action space size 91 action 85\n",
      "episode 14 step 32 reward 9.094947017729282e-13 action space size 92 action 80\n",
      "episode 14 step 33 reward 1.1368683772161603e-12 action space size 92 action 15\n",
      "episode 14 step 34 reward 6.821210263296962e-13 action space size 94 action 0\n",
      "episode 14 step 35 reward 0.0 action space size 95 action 2\n",
      "episode 14 step 36 reward 2.2737367544323206e-13 action space size 96 action 44\n",
      "episode 14 step 37 reward 4.547473508864641e-13 action space size 95 action 0\n",
      "episode 14 step 38 reward 6.821210263296962e-13 action space size 97 action 10\n",
      "episode 14 step 39 reward 0.0 action space size 100 action 73\n",
      "episode 14 step 40 reward 2.2737367544323206e-13 action space size 100 action 6\n",
      "episode 14 step 41 reward 2.2737367544323206e-13 action space size 99 action 67\n",
      "episode 14 step 42 reward 0.0 action space size 99 action 73\n",
      "episode 14 step 43 reward 4.547473508864641e-13 action space size 100 action 76\n",
      "episode 14 step 44 reward 9.094947017729282e-13 action space size 101 action 36\n",
      "episode 14 step 45 reward 6.821210263296962e-13 action space size 103 action 43\n",
      "episode 14 step 46 reward 2.2737367544323206e-13 action space size 102 action 48\n",
      "episode 14 step 47 reward 0.0 action space size 105 action 58\n",
      "episode 14 step 48 reward 4.547473508864641e-13 action space size 106 action 3\n",
      "episode 14 step 49 reward 6.821210263296962e-13 action space size 109 action 21\n",
      "episode 15 step 0 reward 7.750954409857513e-05 action space size 60 action 2\n",
      "episode 15 step 1 reward 0.00010547318834142061 action space size 61 action 52\n",
      "episode 15 step 2 reward 0.01597876321829972 action space size 62 action 42\n",
      "episode 15 step 3 reward 0.009897073704905779 action space size 64 action 18\n",
      "episode 15 step 4 reward 0.00798227064024104 action space size 65 action 27\n",
      "episode 15 step 5 reward 4.4127593355369754e-05 action space size 67 action 23\n",
      "episode 15 step 6 reward 0.004173064476617583 action space size 65 action 23\n",
      "episode 15 step 7 reward 2.9746858672297094e-05 action space size 67 action 35\n",
      "episode 15 step 8 reward 0.000116649084702658 action space size 70 action 41\n",
      "episode 15 step 9 reward 4.32282940892037e-05 action space size 71 action 13\n",
      "episode 15 step 10 reward 4.991367268303293e-05 action space size 71 action 19\n",
      "episode 15 step 11 reward 0.005203793463351758 action space size 73 action 3\n",
      "episode 15 step 12 reward 3.923616486645187e-05 action space size 72 action 9\n",
      "episode 15 step 13 reward 0.0003279334873695916 action space size 75 action 46\n",
      "episode 15 step 14 reward 0.005764045197338419 action space size 73 action 69\n",
      "episode 15 step 15 reward 0.0004227287686262571 action space size 76 action 17\n",
      "episode 15 step 16 reward 0.0006910670585966727 action space size 78 action 6\n",
      "episode 15 step 17 reward 0.0010535036640249018 action space size 78 action 54\n",
      "episode 15 step 18 reward 0.0010798734524541942 action space size 80 action 22\n",
      "episode 15 step 19 reward 0.00013275596575113013 action space size 78 action 40\n",
      "episode 15 step 20 reward 0.0018022206850218936 action space size 81 action 10\n",
      "episode 15 step 21 reward 4.4839506244898075e-05 action space size 80 action 47\n",
      "episode 15 step 22 reward 0.0001077117472050304 action space size 81 action 74\n",
      "episode 15 step 23 reward 0.0001210254013130907 action space size 84 action 4\n",
      "episode 15 step 24 reward 0.0007517163548982353 action space size 85 action 71\n",
      "episode 15 step 25 reward 1.84187056220253e-05 action space size 87 action 6\n",
      "episode 15 step 26 reward 9.837393145062379e-05 action space size 88 action 27\n",
      "episode 15 step 27 reward 0.00025356966307299444 action space size 89 action 15\n",
      "episode 15 step 28 reward 0.0003119949392385024 action space size 87 action 39\n",
      "episode 15 step 29 reward 0.0002146674933101167 action space size 88 action 23\n",
      "episode 15 step 30 reward 7.762185941828648e-05 action space size 91 action 76\n",
      "episode 15 step 31 reward 0.00042887062181762303 action space size 91 action 42\n",
      "episode 15 step 32 reward 1.850997250585351e-05 action space size 92 action 75\n",
      "episode 15 step 33 reward 5.883942958462285e-05 action space size 92 action 50\n",
      "episode 15 step 34 reward 1.025771007334697e-05 action space size 95 action 82\n",
      "episode 15 step 35 reward 0.00017899587146530394 action space size 96 action 19\n",
      "episode 15 step 36 reward 0.0003417092234485608 action space size 97 action 66\n",
      "episode 15 step 37 reward 2.3216017780214315e-05 action space size 96 action 87\n",
      "episode 15 step 38 reward 6.488374037871836e-05 action space size 96 action 80\n",
      "episode 15 step 39 reward 4.965495236319839e-05 action space size 99 action 30\n",
      "episode 15 step 40 reward 3.2599053611193085e-05 action space size 99 action 32\n",
      "episode 15 step 41 reward 6.157339385026717e-05 action space size 99 action 46\n",
      "episode 15 step 42 reward 0.0003040211313418695 action space size 102 action 21\n",
      "episode 15 step 43 reward 6.308768888629857e-05 action space size 100 action 79\n",
      "episode 15 step 44 reward 8.688741218065843e-05 action space size 103 action 23\n",
      "episode 15 step 45 reward 8.219491064664908e-06 action space size 105 action 25\n",
      "episode 15 step 46 reward 1.4116435522737447e-05 action space size 106 action 101\n",
      "episode 15 step 47 reward 2.4369157472392544e-05 action space size 106 action 91\n",
      "episode 15 step 48 reward 2.59596608884749e-05 action space size 105 action 27\n",
      "episode 15 step 49 reward 0.00010352221806897433 action space size 108 action 65\n",
      "episode 16 step 0 reward 0.03093269200417126 action space size 62 action 29\n",
      "episode 16 step 1 reward 0.005907227557145234 action space size 60 action 61\n",
      "episode 16 step 2 reward 0.007319492716305831 action space size 63 action 7\n",
      "episode 16 step 3 reward 0.0005336699496183428 action space size 65 action 48\n",
      "episode 16 step 4 reward 0.000495346263051033 action space size 65 action 37\n",
      "episode 16 step 5 reward 0.023208925090330013 action space size 66 action 33\n",
      "episode 16 step 6 reward 0.03552751194138182 action space size 67 action 21\n",
      "episode 16 step 7 reward 0.0052221192463548505 action space size 67 action 62\n",
      "episode 16 step 8 reward 0.004938259206483053 action space size 69 action 47\n",
      "episode 16 step 9 reward 2.506195869500516e-05 action space size 70 action 38\n",
      "episode 16 step 10 reward 0.001621228179828904 action space size 71 action 64\n",
      "episode 16 step 11 reward 0.0005466336315294029 action space size 72 action 33\n",
      "episode 16 step 12 reward 0.006006064614666684 action space size 72 action 46\n",
      "episode 16 step 13 reward 0.003998600418071874 action space size 72 action 47\n",
      "episode 16 step 14 reward 5.657335123032681e-05 action space size 74 action 59\n",
      "episode 16 step 15 reward 0.0005254804814285308 action space size 74 action 37\n",
      "episode 16 step 16 reward 0.0028425002724361548 action space size 77 action 9\n",
      "episode 16 step 17 reward 0.0014331689621940313 action space size 79 action 3\n",
      "episode 16 step 18 reward 0.001616051284599962 action space size 79 action 15\n",
      "episode 16 step 19 reward 0.0005507113428393495 action space size 81 action 58\n",
      "episode 16 step 20 reward 0.0034655482768357615 action space size 82 action 17\n",
      "episode 16 step 21 reward 0.000374783860934258 action space size 82 action 78\n",
      "episode 16 step 22 reward 7.0768828663858585e-06 action space size 83 action 13\n",
      "episode 16 step 23 reward 9.695992957858834e-06 action space size 82 action 49\n",
      "episode 16 step 24 reward 0.0009013114640765707 action space size 83 action 42\n",
      "episode 16 step 25 reward 0.0015975975748006022 action space size 86 action 15\n",
      "episode 16 step 26 reward 0.001202858711621957 action space size 88 action 58\n",
      "episode 16 step 27 reward 9.832114164964878e-05 action space size 89 action 43\n",
      "episode 16 step 28 reward 9.024023256642977e-06 action space size 87 action 87\n",
      "episode 16 step 29 reward 0.0013022066796111176 action space size 87 action 26\n",
      "episode 16 step 30 reward 9.954776032827795e-06 action space size 91 action 19\n",
      "episode 16 step 31 reward 2.5174926577165024e-05 action space size 90 action 16\n",
      "episode 16 step 32 reward 6.871150753795519e-05 action space size 89 action 27\n",
      "episode 16 step 33 reward 1.257129042642191e-06 action space size 93 action 51\n",
      "episode 16 step 34 reward 0.0005744350651184504 action space size 92 action 10\n",
      "episode 16 step 35 reward 2.705764882193762e-05 action space size 95 action 62\n",
      "episode 16 step 36 reward 6.893100817251252e-06 action space size 94 action 90\n",
      "episode 16 step 37 reward 9.87178236755426e-06 action space size 96 action 51\n",
      "episode 16 step 38 reward 3.0676942515128758e-06 action space size 99 action 79\n",
      "episode 16 step 39 reward 0.0003035762301806244 action space size 100 action 13\n",
      "episode 16 step 40 reward 2.07521952688694e-06 action space size 101 action 6\n",
      "episode 16 step 41 reward 1.7886911791720195e-05 action space size 102 action 11\n",
      "episode 16 step 42 reward 0.00043824678050441435 action space size 101 action 97\n",
      "episode 16 step 43 reward 0.0005064051210865728 action space size 103 action 37\n",
      "episode 16 step 44 reward 7.971708328113891e-06 action space size 103 action 40\n",
      "episode 16 step 45 reward 3.6148135222902056e-06 action space size 106 action 23\n",
      "episode 16 step 46 reward 3.878602001350373e-06 action space size 103 action 32\n",
      "episode 16 step 47 reward 2.4067634058155818e-05 action space size 108 action 6\n",
      "episode 16 step 48 reward 5.941169092693599e-06 action space size 109 action 68\n",
      "episode 16 step 49 reward 3.143424373774906e-05 action space size 106 action 23\n",
      "episode 17 step 0 reward 0.04109836914835796 action space size 62 action 41\n",
      "episode 17 step 1 reward 0.044247057891880104 action space size 62 action 59\n",
      "episode 17 step 2 reward 0.029078210136049165 action space size 64 action 31\n",
      "episode 17 step 3 reward 0.010473569432406293 action space size 65 action 55\n",
      "episode 17 step 4 reward 0.005075271941223036 action space size 64 action 15\n",
      "episode 17 step 5 reward 0.023397288094429314 action space size 64 action 43\n",
      "episode 17 step 6 reward 0.0003139338530218083 action space size 65 action 26\n",
      "episode 17 step 7 reward 0.0038795334990027186 action space size 67 action 15\n",
      "episode 17 step 8 reward 0.003728832353544931 action space size 68 action 57\n",
      "episode 17 step 9 reward 0.029428702885525126 action space size 70 action 15\n",
      "episode 17 step 10 reward 0.0014684915213365457 action space size 70 action 49\n",
      "episode 17 step 11 reward 0.0014275163800903101 action space size 73 action 32\n",
      "episode 17 step 12 reward 0.003186752281862937 action space size 69 action 47\n",
      "episode 17 step 13 reward 0.003998911192866217 action space size 75 action 51\n",
      "episode 17 step 14 reward 0.002928284120571334 action space size 75 action 70\n",
      "episode 17 step 15 reward 0.0037017483932686446 action space size 75 action 13\n",
      "episode 17 step 16 reward 0.0005764983693552495 action space size 74 action 33\n",
      "episode 17 step 17 reward 0.0001168442147445603 action space size 78 action 28\n",
      "episode 17 step 18 reward 6.971943321332219e-06 action space size 79 action 58\n",
      "episode 17 step 19 reward 2.8848866577391163e-05 action space size 77 action 58\n",
      "episode 17 step 20 reward 1.936196963470138e-05 action space size 78 action 51\n",
      "episode 17 step 21 reward 7.652578756278672e-05 action space size 83 action 10\n",
      "episode 17 step 22 reward 0.0003038652894247207 action space size 83 action 10\n",
      "episode 17 step 23 reward 0.0008303786617034348 action space size 84 action 35\n",
      "episode 17 step 24 reward 0.0007710818485975324 action space size 84 action 78\n",
      "episode 17 step 25 reward 0.00013750377070209652 action space size 86 action 41\n",
      "episode 17 step 26 reward 0.0007198640473689011 action space size 85 action 51\n",
      "episode 17 step 27 reward 0.00034561073448458046 action space size 86 action 77\n",
      "episode 17 step 28 reward 0.00015430878920597024 action space size 88 action 73\n",
      "episode 17 step 29 reward 5.2270596597736585e-05 action space size 89 action 27\n",
      "episode 17 step 30 reward 1.8024062001131824e-05 action space size 90 action 7\n",
      "episode 17 step 31 reward 0.0004342286761129799 action space size 92 action 35\n",
      "episode 17 step 32 reward 7.866938403822132e-06 action space size 93 action 48\n",
      "episode 17 step 33 reward 7.534733185821096e-06 action space size 92 action 19\n",
      "episode 17 step 34 reward 2.5463616566412384e-06 action space size 93 action 10\n",
      "episode 17 step 35 reward 1.3702856449526735e-07 action space size 97 action 61\n",
      "episode 17 step 36 reward 6.379525530064711e-07 action space size 96 action 52\n",
      "episode 17 step 37 reward 3.0669830266560894e-06 action space size 96 action 39\n",
      "episode 17 step 38 reward 8.39527820062358e-06 action space size 97 action 13\n",
      "episode 17 step 39 reward 1.0330133136449149e-05 action space size 100 action 80\n",
      "episode 17 step 40 reward 4.220981622893305e-05 action space size 99 action 93\n",
      "episode 17 step 41 reward 0.00012166070973762544 action space size 99 action 91\n",
      "episode 17 step 42 reward 8.915905027606641e-06 action space size 102 action 81\n",
      "episode 17 step 43 reward 4.472691671253415e-06 action space size 102 action 33\n",
      "episode 17 step 44 reward 7.11319705715141e-05 action space size 104 action 58\n",
      "episode 17 step 45 reward 7.635662768734619e-09 action space size 107 action 38\n",
      "episode 17 step 46 reward 4.605524281942053e-07 action space size 107 action 80\n",
      "episode 17 step 47 reward 1.5251137028826633e-06 action space size 106 action 42\n",
      "episode 17 step 48 reward 6.627436505368678e-06 action space size 108 action 31\n",
      "episode 17 step 49 reward 4.049401240990846e-06 action space size 109 action 13\n",
      "episode 18 step 0 reward 0.02306766216088363 action space size 59 action 54\n",
      "episode 18 step 1 reward 0.008878693968199514 action space size 63 action 5\n",
      "episode 18 step 2 reward 0.015601848298047116 action space size 64 action 6\n",
      "episode 18 step 3 reward 0.009065556815130549 action space size 65 action 9\n",
      "episode 18 step 4 reward 0.0011024480422747729 action space size 63 action 50\n",
      "episode 18 step 5 reward 0.015057457358125248 action space size 65 action 47\n",
      "episode 18 step 6 reward 0.0018729053531387763 action space size 68 action 9\n",
      "episode 18 step 7 reward 0.006564346768300311 action space size 68 action 66\n",
      "episode 18 step 8 reward 2.221915201516822e-05 action space size 69 action 51\n",
      "episode 18 step 9 reward 5.464384685183177e-05 action space size 69 action 6\n",
      "episode 18 step 10 reward 0.00025640114517955226 action space size 70 action 52\n",
      "episode 18 step 11 reward 0.0032939176307991147 action space size 71 action 44\n",
      "episode 18 step 12 reward 0.00016720656140023493 action space size 69 action 69\n",
      "episode 18 step 13 reward 0.000895818895969569 action space size 74 action 26\n",
      "episode 18 step 14 reward 0.0009562872878632334 action space size 74 action 31\n",
      "episode 18 step 15 reward 0.0007190860146693012 action space size 77 action 31\n",
      "episode 18 step 16 reward 0.0007913086651569756 action space size 77 action 55\n",
      "episode 18 step 17 reward 0.002230158394013415 action space size 79 action 72\n",
      "episode 18 step 18 reward 0.0002767630994640058 action space size 76 action 74\n",
      "episode 18 step 19 reward 0.00028343151234366815 action space size 78 action 59\n",
      "episode 18 step 20 reward 0.0008567407885493594 action space size 81 action 47\n",
      "episode 18 step 21 reward 0.00018792753326124512 action space size 81 action 32\n",
      "episode 18 step 22 reward 0.00041130284398605 action space size 84 action 57\n",
      "episode 18 step 23 reward 0.00018119883998224395 action space size 82 action 5\n",
      "episode 18 step 24 reward 6.353534809022676e-05 action space size 85 action 4\n",
      "episode 18 step 25 reward 0.0009122898932218959 action space size 83 action 19\n",
      "episode 18 step 26 reward 1.5009600247140042e-05 action space size 86 action 68\n",
      "episode 18 step 27 reward 1.4907597233104752e-05 action space size 88 action 77\n",
      "episode 18 step 28 reward 3.516427250360721e-05 action space size 89 action 87\n",
      "episode 18 step 29 reward 0.00012303686980885686 action space size 90 action 19\n",
      "episode 18 step 30 reward 0.0002389714522905706 action space size 90 action 74\n",
      "episode 18 step 31 reward 7.931916070447187e-05 action space size 89 action 60\n",
      "episode 18 step 32 reward 4.399466661197948e-05 action space size 91 action 66\n",
      "episode 18 step 33 reward 0.0002235939837191836 action space size 94 action 57\n",
      "episode 18 step 34 reward 0.00018722356162470533 action space size 95 action 54\n",
      "episode 18 step 35 reward 2.6090099709108472e-05 action space size 97 action 66\n",
      "episode 18 step 36 reward 0.00013904839624956367 action space size 98 action 79\n",
      "episode 18 step 37 reward 1.093729861167958e-05 action space size 97 action 7\n",
      "episode 18 step 38 reward 7.577370979561238e-05 action space size 98 action 43\n",
      "episode 18 step 39 reward 0.00015157529105636058 action space size 99 action 33\n",
      "episode 18 step 40 reward 3.804895459325053e-05 action space size 99 action 4\n",
      "episode 18 step 41 reward 4.472798082133522e-05 action space size 100 action 64\n",
      "episode 18 step 42 reward 0.00019659027566376608 action space size 101 action 11\n",
      "episode 18 step 43 reward 3.317270147817908e-05 action space size 105 action 41\n",
      "episode 18 step 44 reward 7.373524067588733e-05 action space size 106 action 52\n",
      "episode 18 step 45 reward 1.3055740055278875e-05 action space size 104 action 2\n",
      "episode 18 step 46 reward 1.4967152765166247e-05 action space size 107 action 92\n",
      "episode 18 step 47 reward 2.6872294256463647e-07 action space size 109 action 33\n",
      "episode 18 step 48 reward 4.607224582287017e-06 action space size 109 action 102\n",
      "episode 18 step 49 reward 1.7677109099167865e-05 action space size 107 action 68\n",
      "episode 19 step 0 reward 0.013999254617601764 action space size 60 action 45\n",
      "episode 19 step 1 reward 0.06704303981928206 action space size 63 action 53\n",
      "episode 19 step 2 reward 0.008587765365518862 action space size 60 action 61\n",
      "episode 19 step 3 reward 0.01798323274942959 action space size 62 action 57\n",
      "episode 19 step 4 reward 0.0011595920643685531 action space size 65 action 20\n",
      "episode 19 step 5 reward 0.0017594477392322005 action space size 66 action 30\n",
      "episode 19 step 6 reward 0.0075552903190327925 action space size 67 action 16\n",
      "episode 19 step 7 reward 0.0017543644300985761 action space size 66 action 56\n",
      "episode 19 step 8 reward 0.00045395741653919686 action space size 68 action 34\n",
      "episode 19 step 9 reward 0.0022622045144089498 action space size 68 action 56\n",
      "episode 19 step 10 reward 0.003170544566501121 action space size 72 action 21\n",
      "episode 19 step 11 reward 0.0002606110842862108 action space size 72 action 35\n",
      "episode 19 step 12 reward 0.0010692122575619578 action space size 74 action 27\n",
      "episode 19 step 13 reward 0.0005861929405455157 action space size 73 action 70\n",
      "episode 19 step 14 reward 0.0006593704995339067 action space size 73 action 33\n",
      "episode 19 step 15 reward 0.0022923176200038142 action space size 76 action 17\n",
      "episode 19 step 16 reward 0.0007582754280974768 action space size 77 action 12\n",
      "episode 19 step 17 reward 0.0005733017185320932 action space size 79 action 64\n",
      "episode 19 step 18 reward 0.0007293214889614319 action space size 77 action 9\n",
      "episode 19 step 19 reward 0.0016906584028220095 action space size 77 action 34\n",
      "episode 19 step 20 reward 5.2253411240599235e-05 action space size 82 action 36\n",
      "episode 19 step 21 reward 0.00034650443990358326 action space size 81 action 79\n",
      "episode 19 step 22 reward 0.0007491608603231725 action space size 83 action 71\n",
      "episode 19 step 23 reward 0.979568065929243 action space size 83 action 0\n",
      "episode 19 step 24 reward 9.094947017729282e-13 action space size 85 action 80\n",
      "episode 19 step 25 reward 6.821210263296962e-13 action space size 84 action 69\n",
      "episode 19 step 26 reward 4.547473508864641e-13 action space size 84 action 66\n",
      "episode 19 step 27 reward 4.547473508864641e-13 action space size 86 action 26\n",
      "episode 19 step 28 reward 4.547473508864641e-13 action space size 89 action 54\n",
      "episode 19 step 29 reward 2.2737367544323206e-13 action space size 86 action 35\n",
      "episode 19 step 30 reward 6.821210263296962e-13 action space size 89 action 48\n",
      "episode 19 step 31 reward 4.547473508864641e-13 action space size 92 action 25\n",
      "episode 19 step 32 reward 4.547473508864641e-13 action space size 91 action 29\n",
      "episode 19 step 33 reward 4.547473508864641e-13 action space size 91 action 18\n",
      "episode 19 step 34 reward 2.2737367544323206e-13 action space size 94 action 40\n",
      "episode 19 step 35 reward 0.0 action space size 95 action 11\n",
      "episode 19 step 36 reward 0.0 action space size 97 action 4\n",
      "episode 19 step 37 reward 2.2737367544323206e-13 action space size 96 action 25\n",
      "episode 19 step 38 reward 4.547473508864641e-13 action space size 98 action 10\n",
      "episode 19 step 39 reward 2.2737367544323206e-13 action space size 96 action 53\n",
      "episode 19 step 40 reward 6.821210263296962e-13 action space size 100 action 30\n",
      "episode 19 step 41 reward 6.821210263296962e-13 action space size 100 action 19\n",
      "episode 19 step 42 reward 0.0 action space size 100 action 18\n",
      "episode 19 step 43 reward 2.2737367544323206e-13 action space size 103 action 53\n",
      "episode 19 step 44 reward 2.2737367544323206e-13 action space size 102 action 28\n",
      "episode 19 step 45 reward 2.2737367544323206e-13 action space size 101 action 74\n",
      "episode 19 step 46 reward 0.0 action space size 106 action 20\n",
      "episode 19 step 47 reward 4.547473508864641e-13 action space size 102 action 98\n",
      "episode 19 step 48 reward 9.094947017729282e-13 action space size 109 action 94\n",
      "episode 19 step 49 reward 0.0 action space size 107 action 25\n"
     ]
    }
   ],
   "source": [
    "import gymenv_v2\n",
    "from gymenv_v2 import make_multiple_env\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "run=wandb.init(project=\"finalproject\", entity=\"orcs4529\", tags=[\"training-easy\"])\n",
    "#run=wandb.init(project=\"finalproject\", entity=\"orcs4529\", tags=[\"training-hard\"])\n",
    "#run=wandb.init(project=\"finalproject\", entity=\"orcs4529\", tags=[\"test\"])\n",
    "\n",
    "### TRAINING\n",
    "\n",
    "# Setup: You may generate your own instances on which you train the cutting agent.\n",
    "custom_config = {\n",
    "    \"load_dir\"        : 'instances/randomip_n60_m60',   # this is the location of the randomly generated instances (you may specify a different directory)\n",
    "    \"idx_list\"        : list(range(20)),                # take the first 20 instances from the directory\n",
    "    \"timelimit\"       : 50,                             # the maximum horizon length is 50\n",
    "    \"reward_type\"     : 'obj'                           # DO NOT CHANGE reward_type\n",
    "}\n",
    "\n",
    "# Easy Setup: Use the following environment settings. We will evaluate your agent with the same easy config below:\n",
    "easy_config = {\n",
    "    \"load_dir\"        : 'instances/train_10_n60_m60',\n",
    "    \"idx_list\"        : list(range(10)),\n",
    "    \"timelimit\"       : 50,\n",
    "    \"reward_type\"     : 'obj'\n",
    "}\n",
    "\n",
    "# Hard Setup: Use the following environment settings. We will evaluate your agent with the same hard config below:\n",
    "hard_config = {\n",
    "    \"load_dir\"        : 'instances/train_100_n60_m60',\n",
    "    \"idx_list\"        : list(range(99)),\n",
    "    \"timelimit\"       : 50,\n",
    "    \"reward_type\"     : 'obj'\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # create env\n",
    "    env = make_multiple_env(**easy_config) \n",
    "\n",
    "    for e in range(20):\n",
    "        # gym loop\n",
    "        s = env.reset()   # samples a random instance every time env.reset() is called\n",
    "        d = False\n",
    "        t = 0\n",
    "        repisode = 0\n",
    "\n",
    "        while not d:\n",
    "            #Take a random action\n",
    "            a = np.random.randint(0, s[-1].size, 1)            # s[-1].size shows the number of actions, i.e., cuts available at state s\n",
    "            \n",
    "            #simulate the environment to get the next state\n",
    "            s, r, d, _ = env.step(list(a))\n",
    "            print('episode', e, 'step', t, 'reward', r, 'action space size', s[-1].size, 'action', a[0])\n",
    "            \n",
    "            A, b, c0, cuts_a, cuts_b = s\n",
    "            #print(A.shape, b.shape, c0.shape, cuts_a.shape, cuts_b.shape)\n",
    "\n",
    "            t += 1\n",
    "            repisode += r\n",
    "\n",
    "    \t    #wandb logging\n",
    "            wandb.log({\"Training reward (easy config)\" : repisode})\n",
    "\t    #make sure to use the correct tag in wandb.init in the initialization on top\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
