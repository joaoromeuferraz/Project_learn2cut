{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 02:27:09.164242: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-15 02:27:10.719923: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-12-15 02:27:10.720129: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-12-15 02:27:10.720143: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjrferraz\u001b[0m (\u001b[33morcs4529\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/Project_learn2cut/wandb/run-20221215_022713-1dozsq5m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/orcs4529/finalproject/runs/1dozsq5m\" target=\"_blank\">efficient-microwave-3324</a></strong> to <a href=\"https://wandb.ai/orcs4529/finalproject\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gymenv_v2\n",
    "from gymenv_v2 import make_multiple_env\n",
    "import numpy as np\n",
    "from config import custom_config, easy_config, hard_config\n",
    "from layers import Embedding\n",
    "import tensorflow as tf\n",
    "from policy import Policy, RandomPolicy\n",
    "from rollout import rollout, rollout_multiple\n",
    "import tensorflow_probability as tfp\n",
    "from utils import discounted_rewards, AdamOptimizer\n",
    "import os\n",
    "import time\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "run=wandb.init(project=\"finalproject\", entity=\"orcs4529\", tags=[\"training-easy\"])\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training instances, dir instances/train_10_n60_m60 idx 0\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 1\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 2\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 3\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 4\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 5\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 6\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 7\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 8\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 9\n"
     ]
    }
   ],
   "source": [
    "env = make_multiple_env(**easy_config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"results\"):\n",
    "    os.mkdir(\"results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = [64, 64, 64]\n",
    "activations = ['relu', 'relu', 'linear']\n",
    "lr = 0.001  # varies\n",
    "num_episodes = 50 \n",
    "num_trajectories = 5 # varies\n",
    "num_eval = 10\n",
    "delta_std = 0.1 # varies\n",
    "num_cuts = 10\n",
    "gamma = 0.9\n",
    "\n",
    "run_name = \"easy8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"results/{run_name}\"):\n",
    "    os.mkdir(f\"results/{run_name}\")\n",
    "\n",
    "all_params = {\n",
    "    \"units\": units, \"activations\": activations, \"lr\": lr, \"num_episodes\": num_episodes,\n",
    "    \"num_trajectories\": num_trajectories, \"delta_std\": delta_std, \"num_cuts\": num_cuts, \"gamma\": gamma\n",
    "}\n",
    "np.save(f\"results/{run_name}/params\", all_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning rate, num_trajectories, delta_std\n",
    "preset1 = 0.1, 20, 0.20\n",
    "preset2 = 0.01, 15, 0.10\n",
    "preset3 = 0.001, 10, 0.05\n",
    "preset4 = 0.0001, 10, 0.05\n",
    "\n",
    "def get_params(prev_reward):\n",
    "    if prev_reward <= 0.2:\n",
    "        return preset1\n",
    "    elif prev_reward <= 0.4:\n",
    "        return preset2\n",
    "    elif prev_reward <= 0.6:\n",
    "        return preset3\n",
    "    else:\n",
    "        return preset4\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running loop over episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://wandb.ai/orcs4529/finalproject/runs/1dozsq5m?jupyter=true\" style=\"border:none;width:100%;height:420px;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.jupyter.IFrame at 0x7fd668658190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restricted license - for non-production use only - expires 2024-10-28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 02:29:22.669440: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-12-15 02:29:22.669549: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-15 02:29:22.669596: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (learn2cut): /proc/driver/nvidia/version does not exist\n",
      "2022-12-15 02:29:22.670887: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\n",
      "Simulating 20 trajectories...\n",
      "Estimating gradient...\n",
      "Evaluating rewards...\n",
      "Evaluated rewards: 0.0361\n",
      "mean 0.0360644130732453 max 0.08004073855022398 min 0.004521856069004571 std 0.02398206680094919\n",
      "\n",
      "Episode 1\n",
      "Simulating 20 trajectories...\n",
      "Estimating gradient...\n",
      "Evaluating rewards...\n",
      "Evaluated rewards: 0.0358\n",
      "mean 0.035780630161661976 max 0.06121759867482979 min 0.004521856069004571 std 0.021108073492602155\n",
      "\n",
      "Episode 2\n",
      "Simulating 20 trajectories...\n",
      "Estimating gradient...\n",
      "Evaluating rewards...\n",
      "Evaluated rewards: 0.0381\n",
      "mean 0.03813970284618336 max 0.08292670386322243 min 0.004190590828329732 std 0.024634929177128697\n",
      "\n",
      "Episode 3\n",
      "Simulating 20 trajectories...\n",
      "Estimating gradient...\n",
      "Evaluating rewards...\n",
      "Evaluated rewards: 0.8072\n",
      "mean 0.8071792991377151 max 1.1150639397001214 min 0.6072478073176626 std 0.17844180189934322\n",
      "\n",
      "Episode 4\n",
      "Simulating 10 trajectories...\n",
      "Estimating gradient...\n",
      "Evaluating rewards...\n",
      "Evaluated rewards: 0.8479\n",
      "mean 0.8478876753606528 max 1.1153490818651335 min 0.6072478073176626 std 0.19673019273846076\n",
      "\n",
      "Episode 5\n",
      "Simulating 10 trajectories...\n",
      "Estimating gradient...\n",
      "Evaluating rewards...\n",
      "Evaluated rewards: 0.8080\n",
      "mean 0.807982651736802 max 1.1153490818651335 min 0.6072478073176626 std 0.18113180890652797\n",
      "\n",
      "Episode 6\n",
      "Simulating 10 trajectories...\n",
      "Estimating gradient...\n",
      "Evaluating rewards...\n",
      "Evaluated rewards: 0.8285\n",
      "mean 0.8285428827492751 max 1.1153490818651335 min 0.6072478073176626 std 0.17813988575512096\n",
      "\n",
      "Episode 7\n",
      "Simulating 10 trajectories...\n",
      "Estimating gradient...\n",
      "Evaluating rewards...\n",
      "Evaluated rewards: 0.8570\n",
      "mean 0.8569547737596167 max 1.1153490818651335 min 0.6072478073176626 std 0.18767691592295077\n",
      "\n",
      "Episode 8\n",
      "Simulating 10 trajectories...\n",
      "Estimating gradient...\n",
      "Evaluating rewards...\n",
      "Evaluated rewards: 0.8465\n",
      "mean 0.8464838762180079 max 1.1150639397001214 min 0.6072478073176626 std 0.19485784669039347\n",
      "\n",
      "Episode 9\n",
      "Simulating 10 trajectories...\n",
      "Estimating gradient...\n",
      "Evaluating rewards...\n",
      "Evaluated rewards: 0.8300\n",
      "mean 0.8300400744254602 max 1.1153490818651335 min 0.6072478073176626 std 0.20045625821502655\n",
      "\n",
      "Episode 10\n",
      "Simulating 10 trajectories...\n",
      "Estimating gradient...\n",
      "Evaluating rewards...\n",
      "Evaluated rewards: 0.8080\n",
      "mean 0.8079826517365291 max 1.1153490818651335 min 0.6072478073176626 std 0.18113180890668185\n",
      "\n",
      "Episode 11\n",
      "Simulating 10 trajectories...\n",
      "Estimating gradient...\n",
      "Evaluating rewards...\n",
      "Evaluated rewards: 0.8260\n",
      "mean 0.8259521594217631 max 1.1153490818651335 min 0.6072478073176626 std 0.19949619593826773\n",
      "\n",
      "Episode 12\n",
      "Simulating 10 trajectories...\n",
      "Estimating gradient...\n",
      "Evaluating rewards...\n",
      "Evaluated rewards: 0.8892\n",
      "mean 0.8891679839114204 max 1.1153490818651335 min 0.6072478073176626 std 0.20497878763216595\n",
      "\n",
      "Episode 13\n",
      "Simulating 10 trajectories...\n",
      "Estimating gradient...\n",
      "Evaluating rewards...\n",
      "Evaluated rewards: 0.8465\n",
      "mean 0.8465123904348729 max 1.1153490818651335 min 0.6072478073176626 std 0.19489716374133179\n",
      "\n",
      "Episode 14\n",
      "Simulating 10 trajectories...\n",
      "Estimating gradient...\n",
      "Evaluating rewards...\n",
      "Evaluated rewards: 0.8266\n",
      "mean 0.826552605965594 max 1.1153490818651335 min 0.6072478073176626 std 0.19913170763312413\n",
      "\n",
      "Episode 15\n",
      "Simulating 10 trajectories...\n",
      "Estimating gradient...\n",
      "Evaluating rewards...\n",
      "Evaluated rewards: 0.8888\n",
      "mean 0.8888379684536176 max 1.1153490818651335 min 0.6072478073176626 std 0.20533344384523716\n",
      "\n",
      "Episode 16\n",
      "Simulating 10 trajectories...\n",
      "Estimating gradient...\n",
      "Evaluating rewards...\n"
     ]
    }
   ],
   "source": [
    "%%wandb\n",
    "# initialize policy and test\n",
    "policy = Policy(units, activations, lr)\n",
    "s = env.reset()\n",
    "_ = policy.compute_prob(s)\n",
    "\n",
    "# policy = RandomPolicy()\n",
    "# s = env.reset()\n",
    "# _ = policy.compute_prob(s)\n",
    "\n",
    "optimizers = [AdamOptimizer(lr=lr) for _ in range(len(policy.get_weights()))]\n",
    "rewards_record = []\n",
    "prev_reward = 0.\n",
    "\n",
    "for e in range(num_episodes):\n",
    "    lr, num_trajectories, delta_std = get_params(prev_reward)\n",
    "    for i in range(len(optimizers)):\n",
    "        optimizers[i].lr = lr\n",
    "    start_t = time.time()\n",
    "    print(f\"Episode {e}\")\n",
    "    w_orig = policy.get_weights()\n",
    "    \n",
    "    epsilons = []\n",
    "    rewards_table = np.zeros(num_trajectories)\n",
    "    print(f\"Simulating {num_trajectories} trajectories...\")\n",
    "    \n",
    "    for t in range(num_trajectories):\n",
    "        eps = [np.random.randn(*x.shape)*delta_std for x in w_orig]\n",
    "        w_new = [w_orig[i] + eps[i] for i in range(len(w_orig))]\n",
    "        policy.set_weights(w_new)\n",
    "        rewards, states, actions = rollout(env, policy, num_cuts, gamma)\n",
    "        epsilons.append(eps)\n",
    "        rewards_table[t] = np.mean(rewards)\n",
    "    \n",
    "    rewards_table_norm = (rewards_table - np.mean(rewards_table))/(np.std(rewards_table) + 1e-8)\n",
    "\n",
    "    grads = []\n",
    "    print(\"Estimating gradient...\")\n",
    "    for j in range(len(w_orig)):\n",
    "        arr = np.zeros(epsilons[0][j].shape)\n",
    "        for i in range(len(epsilons)):\n",
    "            arr += epsilons[i][j] * rewards_table[i]\n",
    "        arr /= (len(epsilons) * delta_std)\n",
    "        grads.append(arr)\n",
    "    \n",
    "    # new_w = [w_orig[i] - lr*grads[i] for i in range(len(w_orig))]\n",
    "    new_w = [optimizers[i].update(w_orig[i], grads[i]) for i in range(len(w_orig))]\n",
    "    \n",
    "    policy.set_weights(new_w)\n",
    "    print(\"Evaluating rewards...\")\n",
    "    \n",
    "    eval_r, _, _ = rollout_multiple(env, policy, num_eval, 50, gamma=1.)\n",
    "    eval_r = np.array(eval_r).sum(axis=1)\n",
    "    print(\"Evaluated rewards: %.4f\" % np.mean(eval_r))\n",
    "    print('mean',np.mean(eval_r),'max',np.max(eval_r),'min',np.min(eval_r),'std',np.std(eval_r))\n",
    "    print(\"\")\n",
    "    rewards_record.append(np.mean(eval_r))\n",
    "    \n",
    "    \n",
    "    fixedWindow = 10\n",
    "    if len(rewards_record) >= fixedWindow:\n",
    "        movingAverage = np.mean(rewards_record[len(rewards_record) - fixedWindow:len(rewards_record)])\n",
    "    else:\n",
    "        movingAverage = np.mean(rewards_record)\n",
    "    \n",
    "    wandb.log({f\"Average training reward over {num_trajectories} trajectories\": np.mean(eval_r), f\"Training reward moving average ({fixedWindow} episodes)\": movingAverage})\n",
    "    np.save(f\"results/{run_name}/reward{e}\", eval_r)\n",
    "    prev_reward = np.mean(eval_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_envs_eval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%wandb\n",
    "test_rewards = []\n",
    "test_rewards_sum = []\n",
    "movingAverage = 0\n",
    "averageWindow = 10\n",
    "for i in range(num_envs_eval):\n",
    "    rewards, states, actions = rollout(env, policy, 50, 1.)\n",
    "    test_rewards.append(rewards)\n",
    "    test_rewards_sum.append(np.sum(rewards))\n",
    "    if i >= averageWindow:\n",
    "        movingAverage = np.mean(test_rewards_sum[i - averageWindow:i])\n",
    "    \n",
    "    wandb.log(\n",
    "        {\n",
    "            f\"Test training reward\": np.sum(rewards),\n",
    "            f\"Test moving average reward\": movingAverage\n",
    "        }\n",
    "    )\n",
    "np.save(f\"results/{run_name}/eval\", test_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Training using Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cuts = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0:\n",
      "mean 0.03022674249687125 max 0.04331369729237188 min 0.015532067551566798 std 0.01081458051292926\n",
      "\n",
      "Episode 1:\n",
      "mean 0.009053976514660526 max 0.017542173560850367 min 1.7402659977960866e-05 std 0.0071892081363139244\n",
      "\n",
      "Episode 2:\n",
      "mean 0.008553477670708624 max 0.02582539067602243 min 0.0006530065411425312 std 0.008436587447415399\n",
      "\n",
      "Episode 3:\n",
      "mean 0.02555015917074782 max 0.04136805025977598 min 0.010084274502332846 std 0.011244516942115647\n",
      "\n",
      "error in lp iteration\n",
      "Episode 4:\n",
      "mean 1.4543466022587381e-05 max 2.9086932045174763e-05 min 0.0 std 1.4543466022587381e-05\n",
      "\n",
      "error in lp iteration\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_964/3138991476.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_trajectories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mr_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_cuts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Project_learn2cut/rollout.py\u001b[0m in \u001b[0;36mrollout\u001b[0;34m(env, policy, num_cuts, gamma)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Project_learn2cut/gymenv_v2.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_now\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_now\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Project_learn2cut/gymenv_v2.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Project_learn2cut/gymenv_v2.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Project_learn2cut/gymenv_v2.py\u001b[0m in \u001b[0;36m_reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuts_a\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuts_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moldobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuts_a\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuts_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Project_learn2cut/gymenv_v2.py\u001b[0m in \u001b[0;36mcompute_state\u001b[0;34m(A, b, c)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mc_tilde\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mSOLVER\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'GUROBI'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbasis_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGurobiSolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_tilde\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb_tilde\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc_tilde\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mSOLVER\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'SCIPY'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbasis_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mScipyLinProgSolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_tilde\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb_tilde\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc_tilde\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Project_learn2cut/gurobiutils.py\u001b[0m in \u001b[0;36mGurobiSolve\u001b[0;34m(A, b, c, Method)\u001b[0m\n\u001b[1;32m     33\u001b[0m                  \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                  name=\"X\")\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddConstrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvarrange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcrange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMethod\u001b[0m \u001b[0;31m# primal simplex Method = 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m#print('start optimizing...')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msrc/gurobipy/model.pxi\u001b[0m in \u001b[0;36mgurobipy.Model.addConstrs\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Project_learn2cut/gurobiutils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     33\u001b[0m                  \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                  name=\"X\")\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddConstrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvarrange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcrange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMethod\u001b[0m \u001b[0;31m# primal simplex Method = 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m#print('start optimizing...')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Project_learn2cut/gurobiutils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     33\u001b[0m                  \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                  name=\"X\")\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddConstrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvarrange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcrange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMethod\u001b[0m \u001b[0;31m# primal simplex Method = 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m#print('start optimizing...')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initialize policy and test\n",
    "policy = Policy(units, activations, lr=0.01)\n",
    "s = env.reset()\n",
    "_ = policy.compute_prob(s)\n",
    "rewards_record = []\n",
    "\n",
    "for e in range(num_episodes):\n",
    "    weights.append(policy.get_weights())\n",
    "    rewards, states, actions = [], [], []\n",
    "    for t in range(num_trajectories):\n",
    "        r_, s_, a_ = rollout(env, policy, num_cuts, gamma)\n",
    "        rewards.append(r_)\n",
    "        states.append(s_)\n",
    "        actions.append(a_)\n",
    "    \n",
    "    for state, reward, action in zip(states, rewards, actions):\n",
    "        loss, gs = policy.train(state, reward, action)\n",
    "    \n",
    "    # evaluate rewards\n",
    "    eval_r, _, _ = rollout(env, policy, num_cuts, gamma)\n",
    "\n",
    "    print(f\"Episode {e}:\")\n",
    "    print('mean',np.mean(eval_r),'max',np.max(eval_r),'min',np.min(eval_r),'std',np.std(eval_r))\n",
    "    print(\"\")\n",
    "    rewards_record.append(np.mean(eval_r))\n",
    "    \n",
    "#     fixedWindow = 100\n",
    "#     movingAverage = 0\n",
    "#     if len(rewards_record) >= fixedWindow:\n",
    "#         movingAverage = np.mean(rewards_record[len(rewards_record)-fixedWindow:len(rewards_record)-1])\n",
    "        \n",
    "    # wandb.log({\"Training reward\" : float(rewards_record[-1]), \"Training reward moving average\": movingAverage})\n",
    "    # np.save(f\"results/{run_name}/reward{e}\", eval_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'attention_embedding_9/Variable:0' shape=(64,) dtype=float32, numpy=\n",
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -0.01591764,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ], dtype=float32)>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[1][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'attention_embedding_9/Variable:0' shape=(64,) dtype=float32, numpy=\n",
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -0.01591764,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ], dtype=float32)>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[2][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards, states, actions = rollout(env.envs[0], policy=policy, num_cuts=5, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_rewards = discounted_rewards(rewards, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, g = policy.train(states, d_rewards, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, action = states[0], d_rewards[0], actions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = tf.cast(tf.nn.softmax(policy.attention(state), axis=-1), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_onehot = tf.cast(tf.one_hot(action, len(prob)), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=-0.0013493157667902656>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-tf.reduce_mean(reward*tf.reduce_sum(prob * action_onehot, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnew = policy.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'attention_embedding_5/Variable:0' shape=(61, 64) dtype=float32, numpy=\n",
       "array([[ 0.02765853,  0.02115008, -0.04661967, ..., -0.00032009,\n",
       "         0.06772545, -0.15605754],\n",
       "       [ 0.12472709,  0.03117213, -0.0408311 , ...,  0.09616747,\n",
       "        -0.04850346, -0.01543978],\n",
       "       [ 0.11055695,  0.04528916, -0.10836247, ..., -0.07388528,\n",
       "         0.02407323,  0.04287877],\n",
       "       ...,\n",
       "       [ 0.01313807,  0.06044581,  0.01039451, ..., -0.02181788,\n",
       "         0.03599485, -0.0163053 ],\n",
       "       [-0.02584703,  0.0009844 ,  0.05448824, ..., -0.03947347,\n",
       "         0.03312697, -0.06217254],\n",
       "       [-0.06193162,  0.04476308,  0.06298922, ...,  0.06827538,\n",
       "        -0.03702519,  0.0773042 ]], dtype=float32)>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worig[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(61, 64), dtype=float32, numpy=\n",
       " array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(61, 64), dtype=float32, numpy=\n",
       " array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(64,), dtype=float32, numpy=\n",
       " array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "         0.0000000e+00,  3.4412409e-08, -6.1519186e-11,  0.0000000e+00,\n",
       "         4.7996324e-10,  0.0000000e+00,  0.0000000e+00,  7.6437134e-10,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00, -3.2961063e-05,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "         3.7910268e-05,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "         0.0000000e+00, -4.0729553e-08,  5.4172426e-07, -2.4906913e-11,\n",
       "         4.3194204e-09,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "         0.0000000e+00,  1.6649275e-04,  0.0000000e+00,  0.0000000e+00,\n",
       "         0.0000000e+00,  0.0000000e+00,  1.7548169e-05,  0.0000000e+00,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  7.3424404e-09,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(64,), dtype=float32, numpy=\n",
       " array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        -6.0225519e-11,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "         0.0000000e+00,  8.0228189e-04,  0.0000000e+00,  0.0000000e+00,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "         0.0000000e+00, -4.5216678e-12,  0.0000000e+00,  0.0000000e+00,\n",
       "         0.0000000e+00,  2.1616460e-10,  0.0000000e+00, -1.0797396e-06,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(64, 64), dtype=float32, numpy=\n",
       " array([[ 3.7879660e-04,  2.2997498e-04,  1.1743156e-03, ...,\n",
       "          5.6949466e-05,  6.6954101e-04,  7.8211754e-04],\n",
       "        [-3.7879660e-04, -2.2997498e-04, -1.1743156e-03, ...,\n",
       "         -5.6949466e-05, -6.6954101e-04, -7.8211754e-04],\n",
       "        [-3.7879660e-04, -2.2997498e-04, -1.1743156e-03, ...,\n",
       "         -5.6949466e-05, -6.6954101e-04, -7.8211754e-04],\n",
       "        ...,\n",
       "        [-3.7879660e-04, -2.2997498e-04, -1.1743156e-03, ...,\n",
       "         -5.6949466e-05, -6.6954101e-04, -7.8211754e-04],\n",
       "        [ 3.7879660e-04,  2.2997498e-04,  1.1743156e-03, ...,\n",
       "          5.6949466e-05,  6.6954101e-04,  7.8211754e-04],\n",
       "        [-3.7879660e-04, -2.2997498e-04, -1.1743156e-03, ...,\n",
       "         -5.6949466e-05, -6.6954101e-04, -7.8211754e-04]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(64, 64), dtype=float32, numpy=\n",
       " array([[-6.1495230e-05,  1.4693623e-04, -4.1401032e-05, ...,\n",
       "          1.6702779e-06,  2.2197035e-04,  5.6518227e-05],\n",
       "        [-6.1495230e-05,  1.4693623e-04, -4.1401032e-05, ...,\n",
       "          1.6702779e-06,  2.2197035e-04,  5.6518227e-05],\n",
       "        [-6.1495230e-05,  1.4693623e-04, -4.1401032e-05, ...,\n",
       "          1.6702779e-06,  2.2197035e-04,  5.6518227e-05],\n",
       "        ...,\n",
       "        [ 6.1495230e-05, -1.4693623e-04,  4.1401032e-05, ...,\n",
       "         -1.6702779e-06, -2.2197035e-04, -5.6518227e-05],\n",
       "        [-6.1495230e-05,  1.4693623e-04, -4.1401032e-05, ...,\n",
       "          1.6702779e-06,  2.2197035e-04,  5.6518227e-05],\n",
       "        [ 6.1495230e-05, -1.4693623e-04,  4.1401032e-05, ...,\n",
       "         -1.6702779e-06, -2.2197035e-04, -5.6518227e-05]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(64,), dtype=float32, numpy=\n",
       " array([-3.7879657e-04, -2.2997495e-04, -1.1743157e-03, -1.8745275e-04,\n",
       "         6.6493184e-04, -2.0317033e-04, -1.9953134e-03, -8.1861409e-04,\n",
       "        -3.4116165e-04, -1.3410011e-03, -8.8847696e-04, -6.4739061e-04,\n",
       "        -1.9631418e-03,  2.1521465e-03,  1.3464813e-03, -8.7883777e-04,\n",
       "        -1.0461159e-03, -2.3707461e-03,  2.4249432e-04, -8.4524520e-04,\n",
       "        -5.3757883e-04, -6.2896340e-04,  3.0751438e-05,  7.9674541e-04,\n",
       "        -3.3766183e-04, -1.6090828e-04, -1.9756218e-03,  1.5737078e-03,\n",
       "         1.0835323e-03,  3.8037985e-04, -1.2792679e-04, -1.8548846e-03,\n",
       "         3.7073114e-04,  1.2695788e-04, -9.0488105e-04, -9.3950285e-04,\n",
       "        -2.3960997e-04,  2.3759468e-04,  1.8453735e-04, -2.3050314e-04,\n",
       "         3.2945073e-04,  3.2862301e-03, -7.8195141e-04,  5.4428342e-04,\n",
       "        -7.6560536e-04, -2.0351191e-03,  1.0464372e-03, -1.2083410e-03,\n",
       "        -2.5345047e-04, -5.3430686e-04, -9.4425312e-05,  1.1443099e-03,\n",
       "        -6.3493114e-04,  2.2835610e-04,  7.7243161e-04, -1.9923034e-03,\n",
       "         3.9321347e-04, -6.3248916e-04, -1.4999041e-03, -1.3810236e-03,\n",
       "        -5.0695344e-05, -5.6949459e-05, -6.6954107e-04, -7.8211760e-04],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(64,), dtype=float32, numpy=\n",
       " array([ 6.14961609e-05, -1.46935694e-04,  4.14008973e-05, -7.27179577e-06,\n",
       "        -9.52372648e-06,  2.11643055e-07, -3.30750598e-04,  1.27892476e-04,\n",
       "         3.25204339e-04,  1.24298036e-04, -1.34756044e-03,  7.60876574e-05,\n",
       "         6.43526204e-04,  3.87868495e-05,  5.06103970e-06, -1.00114179e-04,\n",
       "         5.31493788e-06,  4.71514650e-05, -4.86408826e-05, -3.03794164e-04,\n",
       "        -2.91030388e-04, -1.46878418e-04, -8.78474675e-05,  1.21804187e-04,\n",
       "         7.71845225e-06,  1.32615678e-05, -2.20944989e-04, -5.01507893e-04,\n",
       "         2.03476520e-05,  4.57668211e-05, -5.72605059e-05,  1.01595651e-04,\n",
       "         4.94072214e-04,  1.30551052e-05,  9.01249005e-05,  4.07102052e-05,\n",
       "        -4.53572720e-05, -3.69272893e-05, -1.74539164e-05, -8.58044950e-06,\n",
       "        -4.47542407e-05,  4.03066166e-04,  4.35328577e-04, -1.72927976e-04,\n",
       "        -2.83882837e-05, -6.43450767e-06,  2.88581243e-04, -1.45618571e-04,\n",
       "        -3.58610414e-05,  1.24053098e-04, -6.16344623e-05, -1.13855698e-04,\n",
       "         1.93423475e-05, -1.18291005e-04, -9.47732478e-04,  2.55383784e-04,\n",
       "        -4.82285395e-05,  3.32906842e-04, -3.58154066e-05,  2.15793843e-05,\n",
       "         8.29342753e-07, -1.67029793e-06, -2.21970957e-04, -5.65173104e-05],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = policy.compute_prob(states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = tf.cast(prob, tf.double)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_onehot = tf.cast(tf.one_hot(actions[0], len(prob)), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(60,), dtype=float64, numpy=\n",
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0.])>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_selected = tf.reduce_sum(prob * action_onehot, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_selected += 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = tfp.distributions.Categorical(probs=prob, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=-4.0943445622221>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.log_prob(actions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 6), dtype=float64, numpy=\n",
       "array([[1.13376639e-03, 3.08404304e-02, 3.76251281e-02, 4.88329207e-02,\n",
       "        5.19279231e-02, 5.60047287e-02],\n",
       "       [1.35804102e-03, 1.33847005e-02, 1.52768998e-02, 2.73561120e-02,\n",
       "        3.28219100e-02, 3.48714200e-02],\n",
       "       [2.89780385e-04, 2.27100842e-03, 7.88102197e-03, 8.04201753e-03,\n",
       "        8.69801831e-03, 9.77143384e-03],\n",
       "       [1.13673226e-02, 3.98705513e-02, 6.95828604e-02, 7.81896013e-02,\n",
       "        8.58904937e-02, 9.09576944e-02],\n",
       "       [2.13645849e-02, 3.41761543e-02, 5.56414986e-02, 5.59696274e-02,\n",
       "        5.63511518e-02, 5.64184296e-02],\n",
       "       [5.00194310e-04, 1.05171533e-03, 4.28482347e-02, 5.16368156e-01,\n",
       "        5.16368156e-01, 5.16368156e-01],\n",
       "       [8.82255726e-02, 9.50893077e-02, 1.04324512e-01, 1.04337391e-01,\n",
       "        1.12804798e-01, 1.15944789e-01],\n",
       "       [9.45784145e-03, 1.23628241e-02, 2.51291122e-02, 2.62451570e-02,\n",
       "        2.72499772e-02, 2.74800479e-02],\n",
       "       [9.77637083e-05, 1.24941829e-04, 1.21762357e-02, 1.22421749e-02,\n",
       "        1.43840159e-02, 1.48113484e-02],\n",
       "       [2.39412878e-04, 5.96320237e-03, 7.34298953e-03, 1.43408147e-02,\n",
       "        3.53632778e-02, 3.92470075e-02]])>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.convert_to_tensor(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.023704249999999996"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize policy and test\n",
    "policy = Policy(units, activations)\n",
    "s = env.reset()\n",
    "_ = policy.compute_prob(s)\n",
    "rewards_record = []\n",
    "\n",
    "for e in range(num_episodes):\n",
    "    s = env.reset()\n",
    "    w_orig_cons, w_orig_cuts = policy.get_weights()\n",
    "    for t in range(num_trajectories):\n",
    "        epsilon_cons = [np.random.randn(*x.shape)*delta_std for x in w_orig_cons]\n",
    "        epsilon_cuts = [np.random.randn(*x.shape)*delta_std for x in w_orig_cuts]\n",
    "        w_new_cons = [w_orig_cons[i] + epsilon_cons[i] for i in range(len(w_orig_cons))]\n",
    "        w_new_cuts = [w_orig_cuts[i] + epsilon_cuts[i] for i in range(len(w_orig_cuts))]\n",
    "        policy.set_weights(w_new_cons, w_new_cuts)\n",
    "        rewards, times = rollout_env(env=env.envs, policy=policy, num_rollouts=1, rollout_length=rollout_length, gamma=gamma)\n",
    "        epsilon_table_cons.append(epsilon_cons)\n",
    "        epsilon_table_cuts.append(epsilon_cuts)\n",
    "        # epsilon_table.append(epsilon)\n",
    "        train_rewards_table.append(np.mean(rewards))\n",
    "    \n",
    "    train_rewards_table = np.array(train_rewards_table)\n",
    "    train_rewards_table = (train_rewards_table - np.mean(train_rewards_table))/ (np.std(train_rewards_table) + 1e-8)\n",
    "\n",
    "    grads_cons = []\n",
    "    grads_cuts = []\n",
    "    for j in range(len(w_orig_cons)):\n",
    "        arr_cons = np.zeros(epsilon_table_cons[0][j].shape)\n",
    "        arr_cuts = np.zeros(epsilon_table_cuts[0][j].shape)\n",
    "        for i in range(len(epsilon_table_cons)):\n",
    "            arr_cons += epsilon_table_cons[i][j] * train_rewards_table[i]\n",
    "            arr_cuts += epsilon_table_cuts[i][j] * train_rewards_table[i]\n",
    "        arr_cons /= (len(epsilon_table_cons) * delta_std)\n",
    "        arr_cuts /= (len(epsilon_table_cuts) * delta_std)\n",
    "        grads_cons.append(arr_cons)\n",
    "        grads_cuts.append(arr_cuts)\n",
    "    \n",
    "    # assign back original weights and update\n",
    "    w_cons = [w_orig_cons[i] - lr*grads_cons[i] for i in range(len(w_orig_cons))]\n",
    "    w_cuts = [w_orig_cuts[i] - lr*grads_cuts[i] for i in range(len(w_orig_cuts))]\n",
    "\n",
    "    policy.set_weights(w_cons, w_cuts)\n",
    "    \n",
    "    # evaluate rewards\n",
    "    eval_r, _ = rollout_envs(envs=env.envs, policy=policy, num_rollouts=1, rollout_length=time_limit, gamma=gamma)\n",
    "    print(f\"Episode {e}:\")\n",
    "    print('mean',np.mean(eval_r),'max',np.max(eval_r),'min',np.min(eval_r),'std',np.std(eval_r))\n",
    "    print(\"\")\n",
    "    rewards_record.append(np.mean(eval_r))\n",
    "    \n",
    "    fixedWindow = 100\n",
    "    movingAverage = 0\n",
    "    if len(rewards_record) >= fixedWindow:\n",
    "        movingAverage = np.mean(rewards_record[len(rewards_record)-fixedWindow:len(rewards_record)-1])\n",
    "        \n",
    "    wandb.log({\"Training reward\" : float(rewards_record[-1]), \"Training reward moving average\": movingAverage})\n",
    "    np.save(f\"results/{run_name}/reward{e}\", eval_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0032"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1-(0.6**2))/200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m100",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m100"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
