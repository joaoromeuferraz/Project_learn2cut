{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjrferraz\u001b[0m (\u001b[33morcs4529\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/joaoromeuferraz/Documents/GitHub/Project_learn2cut/wandb/run-20221209_193425-3lp48jft</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/orcs4529/finalproject/runs/3lp48jft\" target=\"_blank\">ethereal-monkey-1342</a></strong> to <a href=\"https://wandb.ai/orcs4529/finalproject\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gymenv_v2\n",
    "from gymenv_v2 import make_multiple_env\n",
    "import numpy as np\n",
    "from config import custom_config, easy_config, hard_config\n",
    "from layers import Embedding\n",
    "import tensorflow as tf\n",
    "from policy import Policy\n",
    "from rollout import rollout, rollout_envs\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "run=wandb.init(project=\"finalproject\", entity=\"orcs4529\", tags=[\"training-easy\"])\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training instances, dir instances/train_10_n60_m60 idx 0\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 1\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 2\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 3\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 4\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 5\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 6\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 7\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 8\n",
      "loading training instances, dir instances/train_10_n60_m60 idx 9\n"
     ]
    }
   ],
   "source": [
    "env = make_multiple_env(**easy_config) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = [64, 64]\n",
    "activations = ['tanh', 'tanh']\n",
    "lr = 0.001  ### CHANGE\n",
    "num_episodes = 50\n",
    "num_trajectories = 10 ### CHANGE\n",
    "delta_std = 0.01\n",
    "rollout_length = 1\n",
    "time_limit = 10 ### CHANGE\n",
    "gamma = 0.98"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize policy and test\n",
    "policy = Policy(units, activations)\n",
    "s = env.reset()\n",
    "_ = policy.compute_prob(s)\n",
    "rewards_record = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each episode\n",
    "w_orig_cons, w_orig_cuts = policy.get_weights()\n",
    "# epsilon_table = []\n",
    "epsilon_table_cons, epsilon_table_cuts = [], []\n",
    "train_rewards_table = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each trajectory\n",
    "# epsilon = np.random.randn()*delta_std\n",
    "epsilon_cons = [np.random.randn(*x.shape)*delta_std for x in w_cons]\n",
    "epsilon_cuts = [np.random.randn(*x.shape)*delta_std for x in w_cuts]\n",
    "w_new_cons = [x + epsilon for x in w_orig_cons]\n",
    "w_new_cuts = [x + epsilon for x in w_orig_cons]\n",
    "policy.set_weights(w_new_cons, w_new_cuts)\n",
    "rewards, times = rollout_envs(envs=env.envs, policy=policy, num_rollouts=1, rollout_length=rollout_length, gamma=gamma)\n",
    "epsilon_table_cons.append(epsilon_cons)\n",
    "epsilon_table_cuts.append(epsilon_cuts)\n",
    "# epsilon_table.append(epsilon)\n",
    "train_rewards_table.append(np.mean(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acumulate gradients\n",
    "# epsilon_table = np.array(epsilon_table)\n",
    "train_rewards_table = np.array(train_rewards_table)\n",
    "train_rewards_table = (train_rewards_table - np.mean(train_rewards_table))/ (np.std(train_rewards_table) + 1e-8)\n",
    "\n",
    "grads_cons = []\n",
    "grads_cuts = []\n",
    "for j in range(len(w_orig_cons)):\n",
    "    arr_cons = np.zeros(epsilon_table_cons[0][j].shape)\n",
    "    arr_cuts = np.zeros(epsilon_table_cuts[0][j].shape)\n",
    "    for i in range(len(epsilon_table_cons)):\n",
    "        arr_cons += epsilon_table_cons[i][j] * train_rewards_table[i]\n",
    "        arr_cuts += epsilon_table_cuts[i][j] * train_rewards_table[i]\n",
    "    arr_cons /= (len(epsilon_table_cons) * delta_std)\n",
    "    arr_cuts /= (len(epsilon_table_cuts) * delta_std)\n",
    "    grads_cons.append(arr_cons)\n",
    "    grads_cuts.append(arr_cuts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign back original weights and update\n",
    "w_cons = [w_orig_cons[i] - lr*grads_cons[i] for i in range(len(w_orig_cons))]\n",
    "w_cuts = [w_orig_cuts[i] - lr*grads_cuts[i] for i in range(len(w_orig_cuts))]\n",
    "\n",
    "policy.set_weights(w_cons, w_cuts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate rewards\n",
    "evaluated_rewards, _ = rollout_envs(envs=env.envs, policy=policy, num_rollouts=10, rollout_length=time_limit, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean 0.13378659235710277 max 0.8532251308604202 min 0.012964513668684861 std 0.18596293519985257\n"
     ]
    }
   ],
   "source": [
    "x = evaluated_rewards\n",
    "print('mean',np.mean(x),'max',np.max(x),'min',np.min(x),'std',np.std(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_record.append(np.mean(evaluated_rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running loop over episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:14: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (61,64) (4,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9d4930199619>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mepsilon_cons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdelta_std\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw_orig_cons\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mepsilon_cuts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdelta_std\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw_orig_cuts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mw_new_cons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepsilon_cons\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw_orig_cons\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mw_new_cuts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepsilon_cuts\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw_orig_cons\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_new_cons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_new_cuts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-9d4930199619>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mepsilon_cons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdelta_std\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw_orig_cons\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mepsilon_cuts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdelta_std\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw_orig_cuts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mw_new_cons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepsilon_cons\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw_orig_cons\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mw_new_cuts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepsilon_cuts\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw_orig_cons\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_new_cons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_new_cuts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (61,64) (4,) "
     ]
    }
   ],
   "source": [
    "# initialize policy and test\n",
    "policy = Policy(units, activations)\n",
    "s = env.reset()\n",
    "_ = policy.compute_prob(s)\n",
    "rewards_record = []\n",
    "\n",
    "for e in range(num_episodes):\n",
    "    w_orig_cons, w_orig_cuts = policy.get_weights()\n",
    "    epsilon_table_cons, epsilon_table_cuts = [], []\n",
    "    train_rewards_table = []\n",
    "    for t in range(num_trajectories):\n",
    "        epsilon_cons = [np.random.randn(*x.shape)*delta_std for x in w_orig_cons]\n",
    "        epsilon_cuts = [np.random.randn(*x.shape)*delta_std for x in w_orig_cuts]\n",
    "        w_new_cons = [w_orig_cons[i] + epsilon_cons[i] for i in range(len(w_orig_cons))]\n",
    "        w_new_cuts = [w_orig_cuts[i] + epsilon_cuts[i] for i in range(len(w_orig_cuts))]\n",
    "        policy.set_weights(w_new_cons, w_new_cuts)\n",
    "        rewards, times = rollout_envs(envs=env.envs, policy=policy, num_rollouts=1, rollout_length=rollout_length, gamma=gamma)\n",
    "        epsilon_table_cons.append(epsilon_cons)\n",
    "        epsilon_table_cuts.append(epsilon_cuts)\n",
    "        # epsilon_table.append(epsilon)\n",
    "        train_rewards_table.append(np.mean(rewards))\n",
    "    \n",
    "    train_rewards_table = np.array(train_rewards_table)\n",
    "    train_rewards_table = (train_rewards_table - np.mean(train_rewards_table))/ (np.std(train_rewards_table) + 1e-8)\n",
    "\n",
    "    grads_cons = []\n",
    "    grads_cuts = []\n",
    "    for j in range(len(w_orig_cons)):\n",
    "        arr_cons = np.zeros(epsilon_table_cons[0][j].shape)\n",
    "        arr_cuts = np.zeros(epsilon_table_cuts[0][j].shape)\n",
    "        for i in range(len(epsilon_table_cons)):\n",
    "            arr_cons += epsilon_table_cons[i][j] * train_rewards_table[i]\n",
    "            arr_cuts += epsilon_table_cuts[i][j] * train_rewards_table[i]\n",
    "        arr_cons /= (len(epsilon_table_cons) * delta_std)\n",
    "        arr_cuts /= (len(epsilon_table_cuts) * delta_std)\n",
    "        grads_cons.append(arr_cons)\n",
    "        grads_cuts.append(arr_cuts)\n",
    "    \n",
    "    # assign back original weights and update\n",
    "    w_cons = [w_orig_cons[i] - lr*grads_cons[i] for i in range(len(w_orig_cons))]\n",
    "    w_cuts = [w_orig_cuts[i] - lr*grads_cuts[i] for i in range(len(w_orig_cuts))]\n",
    "\n",
    "    policy.set_weights(w_cons, w_cuts)\n",
    "    \n",
    "    # evaluate rewards\n",
    "    eval_r, _ = rollout_envs(envs=env.envs, policy=policy, num_rollouts=10, rollout_length=time_limit, gamma=gamma)\n",
    "    print(f\"Episode {e}:\")\n",
    "    print('mean',np.mean(eval_r),'max',np.max(eval_r),'min',np.min(eval_r),'std',np.std(eval_r))\n",
    "    print(\"\")\n",
    "    rewards_record.append(np.mean(evaluated_rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61, 64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wpsilon_con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
